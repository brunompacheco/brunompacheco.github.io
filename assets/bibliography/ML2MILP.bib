
@article{bengio_machine_2021,
	title = {Machine learning for combinatorial optimization: A methodological tour d’horizon},
	volume = {290},
	issn = {0377-2217},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221720306895},
	doi = {10.1016/j.ejor.2020.07.063},
	shorttitle = {Machine learning for combinatorial optimization},
	abstract = {This paper surveys the recent attempts, both from the machine learning and operations research communities, at leveraging machine learning to solve combinatorial optimization problems. Given the hard nature of these problems, state-of-the-art algorithms rely on handcrafted heuristics for making decisions that are otherwise too expensive to compute or mathematically not well defined. Thus, machine learning looks like a natural candidate to make such decisions in a more principled and optimized way. We advocate for pushing further the integration of machine learning and combinatorial optimization and detail a methodology to do so. A main point of the paper is seeing generic optimization problems as data points and inquiring what is the relevant distribution of problems to use for learning on a given task.},
	pages = {405--421},
	number = {2},
	journaltitle = {European Journal of Operational Research},
	shortjournal = {European Journal of Operational Research},
	author = {Bengio, Yoshua and Lodi, Andrea and Prouvost, Antoine},
	urldate = {2023-04-19},
	date = {2021-04-16},
	langid = {english},
	note = {[A] {DONE}},
	keywords = {Machine learning, Branch and bound, Combinatorial optimization, Mixed-integer programming solvers, {DONE}, {SURVEY}, *, background},
	file = {ScienceDirect Full Text PDF:/home/brunompacheco/Zotero/storage/FC9ASULK/Bengio et al. - 2021 - Machine learning for combinatorial optimization A.pdf:application/pdf;ScienceDirect Snapshot:/home/brunompacheco/Zotero/storage/ZY24WMP5/S0377221720306895.html:text/html},
}

@article{lodi_learning_2017,
	title = {On learning and branching: a survey},
	volume = {25},
	issn = {1863-8279},
	url = {https://doi.org/10.1007/s11750-017-0451-6},
	doi = {10.1007/s11750-017-0451-6},
	shorttitle = {On learning and branching},
	abstract = {This paper surveys learning techniques to deal with the two most crucial decisions in the branch-and-bound algorithm for Mixed-Integer Linear Programming, namely variable and node selections. Because of the lack of deep mathematical understanding on those decisions, the classical and vast literature in the field is inherently based on computational studies and heuristic, often problem-specific, strategies. We will both interpret some of those early contributions in the light of modern (machine) learning techniques, and give the details of the recent algorithms that instead explicitly incorporate machine learning paradigms.},
	pages = {207--236},
	number = {2},
	journaltitle = {{TOP}},
	shortjournal = {{TOP}},
	author = {Lodi, Andrea and Zarpellon, Giulia},
	urldate = {2023-04-19},
	date = {2017-07-01},
	langid = {english},
	note = {[B]},
	keywords = {Machine learning, Branch and bound, 68R-02, 68T-02, 90-02, {SURVEY}},
	file = {Full Text PDF:/home/brunompacheco/Zotero/storage/JEKZ6HKM/Lodi and Zarpellon - 2017 - On learning and branching a survey.pdf:application/pdf},
}

@article{liberto_dash_2016,
	title = {{DASH}: Dynamic Approach for Switching Heuristics},
	volume = {248},
	issn = {0377-2217},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221715007559},
	doi = {10.1016/j.ejor.2015.08.018},
	shorttitle = {{DASH}},
	abstract = {Complete tree search is a highly effective method for tackling Mixed-Integer Programming ({MIP}) problems, and over the years, a plethora of branching heuristics have been introduced to further refine the technique for varying problems. Yet while each new approach continued to push the state-of-the-art, parallel research began to repeatedly demonstrate that there is no single method that would perform the best on all problem instances. Tackling this issue, portfolio algorithms took the process a step further, by trying to predict the best heuristic for each instance at hand. However, the motivation behind algorithm selection can be taken further still, and used to dynamically choose the most appropriate algorithm for each encountered sub-problem. In this paper we identify a feature space that captures both the evolution of the problem in the branching tree and the similarity among sub-problems of instances from the same {MIP} models. We show how to exploit these features on-the-fly in order to decide the best time to switch the branching variable selection heuristic and then show how such a system can be trained efficiently. Experiments on a highly heterogeneous collection of hard {MIP} instances show significant gains over the standard pure approach which commits to a single heuristic throughout the search.},
	pages = {943--953},
	number = {3},
	journaltitle = {European Journal of Operational Research},
	shortjournal = {European Journal of Operational Research},
	author = {Liberto, Giovanni Di and Kadioglu, Serdar and Leo, Kevin and Malitsky, Yuri},
	urldate = {2023-04-19},
	date = {2016-02-01},
	langid = {english},
	note = {[B] {SKIMMED}},
	keywords = {Algorithm selection, Dynamic search heuristics, Mixed-Integer Programming, {DONE}, *, primal heuristics},
	file = {ScienceDirect Full Text PDF:/home/brunompacheco/Zotero/storage/YBRR7DC6/Liberto et al. - 2016 - DASH Dynamic Approach for Switching Heuristics.pdf:application/pdf;ScienceDirect Snapshot:/home/brunompacheco/Zotero/storage/NTS6ZP84/S0377221715007559.html:text/html},
}

@inproceedings{kruber_learning_2017,
	location = {Cham},
	title = {Learning When to Use a Decomposition},
	isbn = {978-3-319-59776-8},
	doi = {10.1007/978-3-319-59776-8_16},
	series = {Lecture Notes in Computer Science},
	abstract = {Applying a Dantzig-Wolfe decomposition to a mixed-integer program ({MIP}) aims at exploiting an embedded model structure and can lead to significantly stronger reformulations of the {MIP}. Recently, automating the process and embedding it in standard {MIP} solvers have been proposed, with the detection of a decomposable model structure as key element. If the detected structure reflects the (usually unknown) actual structure of the {MIP} well, the solver may be much faster on the reformulated model than on the original. Otherwise, the solver may completely fail. We propose a supervised learning approach to decide whether or not a reformulation should be applied, and which decomposition to choose when several are possible. Preliminary experiments with a {MIP} solver equipped with this knowledge show a significant performance improvement on structured instances, with little deterioration on others.},
	pages = {202--210},
	booktitle = {Integration of AI and OR Techniques in Constraint Programming},
	publisher = {Springer International Publishing},
	author = {Kruber, Markus and Lübbecke, Marco E. and Parmentier, Axel},
	editor = {Salvagnin, Domenico and Lombardi, Michele},
	date = {2017},
	langid = {english},
	note = {[A]},
	keywords = {Mixed-integer programming, Automatic Dantzig-Wolfe decomposition, Branch-and-price, Column generation, Supervised learning, {DONE}, primal heuristics},
	file = {Kruber et al. - 2017 - Learning When to Use a Decomposition.pdf:/home/brunompacheco/Zotero/storage/YGH3QTSD/Kruber et al. - 2017 - Learning When to Use a Decomposition.pdf:application/pdf},
}

@article{baltean-lugojan_selecting_2018,
	title = {Selecting cutting planes for quadratic semidefinite outer-approximation via trained neural networks},
	journaltitle = {{URL}: http://www. optimization-online. org/{DB}\_HTML/2018/11/6943. html},
	author = {Baltean-Lugojan, Radu and Bonami, Pierre and Misener, Ruth and Tramontani, Andrea},
	date = {2018},
	note = {[A]},
	file = {Baltean-Lugojan et al. - 2018 - Selecting cutting planes for quadratic semidefinit.pdf:/home/brunompacheco/Zotero/storage/JLRPNZKC/Baltean-Lugojan et al. - 2018 - Selecting cutting planes for quadratic semidefinit.pdf:application/pdf},
}

@inproceedings{mahmood_automated_2018,
	title = {Automated Treatment Planning in Radiation Therapy using Generative Adversarial Networks},
	url = {https://proceedings.mlr.press/v85/mahmood18a.html},
	abstract = {Knowledge-based planning ({KBP}) is an automated approach to radiation therapy treatment planning that involves predicting desirable treatment plans before they are then corrected to deliverable ones. We propose a generative adversarial network ({GAN}) approach for predicting desirable 3D dose distributions that eschews the previous paradigms of site-specific feature engineering and predicting low-dimensional representations of the plan. Experiments on a dataset of oropharyngeal cancer patients show that our approach significantly outperforms previous methods on several clinical satisfaction criteria and similarity metrics.},
	eventtitle = {Machine Learning for Healthcare Conference},
	pages = {484--499},
	booktitle = {Proceedings of the 3rd Machine Learning for Healthcare Conference},
	publisher = {{PMLR}},
	author = {Mahmood, Rafid and Babier, Aaron and {McNiven}, Andrea and Diamant, Adam and Chan, Timothy C. Y.},
	urldate = {2023-04-25},
	date = {2018-11-29},
	langid = {english},
	note = {[A]},
	keywords = {Pred-and-opt},
	file = {Full Text PDF:/home/brunompacheco/Zotero/storage/BISEQ5YK/Mahmood et al. - 2018 - Automated Treatment Planning in Radiation Therapy .pdf:application/pdf;Supplementary PDF:/home/brunompacheco/Zotero/storage/49INAPFV/Mahmood et al. - 2018 - Automated Treatment Planning in Radiation Therapy .pdf:application/pdf},
}

@misc{gasse_exact_2019,
	title = {Exact Combinatorial Optimization with Graph Convolutional Neural Networks},
	url = {http://arxiv.org/abs/1906.01629},
	doi = {10.48550/arXiv.1906.01629},
	abstract = {Combinatorial optimization problems are typically tackled by the branch-and-bound paradigm. We propose a new graph convolutional neural network model for learning branch-and-bound variable selection policies, which leverages the natural variable-constraint bipartite graph representation of mixed-integer linear programs. We train our model via imitation learning from the strong branching expert rule, and demonstrate on a series of hard problems that our approach produces policies that improve upon state-of-the-art machine-learning methods for branching and generalize to instances significantly larger than seen during training. Moreover, we improve for the first time over expert-designed branching rules implemented in a state-of-the-art solver on large problems. Code for reproducing all the experiments can be found at https://github.com/ds4dm/learn2branch.},
	number = {{arXiv}:1906.01629},
	publisher = {{arXiv}},
	author = {Gasse, Maxime and Chételat, Didier and Ferroni, Nicola and Charlin, Laurent and Lodi, Andrea},
	urldate = {2023-04-27},
	date = {2019-10-30},
	note = {[A]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning, {GNN}, *, imitation learning, background},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/SUJREJ69/Gasse et al. - 2019 - Exact Combinatorial Optimization with Graph Convol.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/GBFURMW2/1906.html:text/html},
}

@misc{prouvost_ecole_2021,
	title = {Ecole: A Library for Learning Inside {MILP} Solvers},
	url = {http://arxiv.org/abs/2104.02828},
	doi = {10.48550/arXiv.2104.02828},
	shorttitle = {Ecole},
	abstract = {In this paper we describe Ecole (Extensible Combinatorial Optimization Learning Environments), a library to facilitate integration of machine learning in combinatorial optimization solvers. It exposes sequential decision making that must be performed in the process of solving as Markov decision processes. This means that, rather than trying to predict solutions to combinatorial optimization problems directly, Ecole allows machine learning to work in cooperation with a state-of-the-art a mixed-integer linear programming solver that acts as a controllable algorithm. Ecole provides a collection of computationally efficient, ready to use learning environments, which are also easy to extend to define novel training tasks. Documentation and code can be found at https://www.ecole.ai.},
	number = {{arXiv}:2104.02828},
	publisher = {{arXiv}},
	author = {Prouvost, Antoine and Dumouchelle, Justin and Gasse, Maxime and Chételat, Didier and Lodi, Andrea},
	urldate = {2023-04-27},
	date = {2021-04-06},
	eprinttype = {arxiv},
	eprint = {2104.02828 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, background},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/6B5TLRNP/Prouvost et al. - 2021 - Ecole A Library for Learning Inside MILP Solvers.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/MB7UKGCZ/2104.html:text/html},
}

@inproceedings{fischetti_learning_2019,
	location = {Cham},
	title = {Learning {MILP} Resolution Outcomes Before Reaching Time-Limit},
	isbn = {978-3-030-19212-9},
	doi = {10.1007/978-3-030-19212-9_18},
	series = {Lecture Notes in Computer Science},
	abstract = {The resolution of some Mixed-Integer Linear Programming ({MILP}) problems still presents challenges for state-of-the-art optimization solvers and may require hours of computations, so that a time-limit to the resolution process is typically provided by a user. Nevertheless, it could be useful to get a sense of the optimization trends after only a fraction of the specified total time has passed, and ideally be able to tailor the use of the remaining resolution time accordingly, in a more strategic and flexible way. Looking at the evolution of a partial branch-and-bound tree for a {MILP} instance, developed up to a certain fraction of the time-limit, we aim to predict whether the problem will be solved to proven optimality before timing out. We exploit machine learning tools, and summarize the development and progress of a {MILP} resolution process to cast a prediction within a classification framework. Experiments on benchmark instances show that a valuable statistical pattern can indeed be learned during {MILP} resolution, with key predictive features reflecting the know-how and experience of field’s practitioners.},
	pages = {275--291},
	booktitle = {Integration of Constraint Programming, Artificial Intelligence, and Operations Research},
	publisher = {Springer International Publishing},
	author = {Fischetti, Martina and Lodi, Andrea and Zarpellon, Giulia},
	editor = {Rousseau, Louis-Martin and Stergiou, Kostas},
	date = {2019},
	langid = {english},
	keywords = {Machine learning, Branch and Bound, {MILP} resolution, imitation learning},
	file = {Fischetti et al. - 2019 - Learning MILP Resolution Outcomes Before Reaching .pdf:/home/brunompacheco/Zotero/storage/SWZ9E4WH/Fischetti et al. - 2019 - Learning MILP Resolution Outcomes Before Reaching .pdf:application/pdf},
}

@article{jimenez-cordero_warm-starting_2022,
	title = {Warm-starting constraint generation for mixed-integer optimization: A Machine Learning approach},
	volume = {253},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122007894},
	doi = {10.1016/j.knosys.2022.109570},
	shorttitle = {Warm-starting constraint generation for mixed-integer optimization},
	abstract = {Mixed Integer Linear Programs ({MILP}) are well known to be {NP}-hard (Non-deterministic Polynomial-time hard) problems in general. Even though pure optimization-based methods, such as constraint generation, are guaranteed to provide an optimal solution if enough time is given, their use in online applications remains a great challenge due to their usual excessive time requirements. To alleviate their computational burden, some machine learning techniques ({ML}) have been proposed in the literature, using the information provided by previously solved {MILP} instances. Unfortunately, these techniques report a non-negligible percentage of infeasible or suboptimal instances. By linking mathematical optimization and machine learning, this paper proposes a novel approach that speeds up the traditional constraint generation method, preserving feasibility and optimality guarantees. In particular, we first identify offline the so-called invariant constraint set of past {MILP} instances. We then train (also offline) a machine learning method to learn an invariant constraint set as a function of the problem parameters of each instance. Next, we predict online an invariant constraint set of the new unseen {MILP} application and use it to initialize the constraint generation method. This warm-started strategy significantly reduces the number of iterations to reach optimality, and therefore, the computational burden to solve online each {MILP} problem is significantly reduced. Very importantly, all the feasibility and optimality theoretical guarantees of the traditional constraint generation method are inherited by our proposed methodology. The computational performance of the proposed approach is quantified through synthetic and real-life {MILP} applications.},
	pages = {109570},
	journaltitle = {Knowledge-Based Systems},
	shortjournal = {Knowledge-Based Systems},
	author = {Jiménez-Cordero, Asunción and Morales, Juan Miguel and Pineda, Salvador},
	urldate = {2023-04-27},
	date = {2022-10-11},
	langid = {english},
	keywords = {Machine learning, Constraint generation, Feasibility and optimality guarantees, Mixed integer linear programming, Warm-start, imitation learning},
	file = {Presentation slides:/home/brunompacheco/Zotero/storage/KTWXDEZ3/MS10-JimenezCorderoAsuncion-s.pdf:application/pdf;ScienceDirect Full Text PDF:/home/brunompacheco/Zotero/storage/QWINDZG6/Jiménez-Cordero et al. - 2022 - Warm-starting constraint generation for mixed-inte.pdf:application/pdf;ScienceDirect Snapshot:/home/brunompacheco/Zotero/storage/CEDEAWZR/S0950705122007894.html:text/html},
}

@misc{chen_representing_2022,
	title = {On Representing Mixed-Integer Linear Programs by Graph Neural Networks},
	url = {http://arxiv.org/abs/2210.10759},
	doi = {10.48550/arXiv.2210.10759},
	abstract = {While Mixed-integer linear programming ({MILP}) is {NP}-hard in general, practical {MILP} has received roughly 100--fold speedup in the past twenty years. Still, many classes of {MILPs} quickly become unsolvable as their sizes increase, motivating researchers to seek new acceleration techniques for {MILPs}. With deep learning, they have obtained strong empirical results, and many results were obtained by applying graph neural networks ({GNNs}) to making decisions in various stages of {MILP} solution processes. This work discovers a fundamental limitation: there exist feasible and infeasible {MILPs} that all {GNNs} will, however, treat equally, indicating {GNN}'s lacking power to express general {MILPs}. Then, we show that, by restricting the {MILPs} to unfoldable ones or by adding random features, there exist {GNNs} that can reliably predict {MILP} feasibility, optimal objective values, and optimal solutions up to prescribed precision. We conducted small-scale numerical experiments to validate our theoretical findings.},
	number = {{arXiv}:2210.10759},
	publisher = {{arXiv}},
	author = {Chen, Ziang and Liu, Jialin and Wang, Xinshang and Lu, Jianfeng and Yin, Wotao},
	urldate = {2023-04-27},
	date = {2022-10-19},
	eprinttype = {arxiv},
	eprint = {2210.10759 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, {GNN}, *, imitation learning, end-to-end},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/6NID6SVY/Chen et al. - 2022 - On Representing Mixed-Integer Linear Programs by G.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/CCGSLRAG/2210.html:text/html},
}

@misc{han_gnn-guided_2023,
	title = {A {GNN}-Guided Predict-and-Search Framework for Mixed-Integer Linear Programming},
	url = {http://arxiv.org/abs/2302.05636},
	doi = {10.48550/arXiv.2302.05636},
	abstract = {Mixed-integer linear programming ({MILP}) is widely employed for modeling combinatorial optimization problems. In practice, similar {MILP} instances with only coefficient variations are routinely solved, and machine learning ({ML}) algorithms are capable of capturing common patterns across these {MILP} instances. In this work, we combine {ML} with optimization and propose a novel predict-and-search framework for efficiently identifying high-quality feasible solutions. Specifically, we first utilize graph neural networks to predict the marginal probability of each variable, and then search for the best feasible solution within a properly defined ball around the predicted solution. We conduct extensive experiments on public datasets, and computational results demonstrate that our proposed framework achieves 51.1\% and 9.9\% performance improvements to {MILP} solvers {SCIP} and Gurobi on primal gaps, respectively.},
	number = {{arXiv}:2302.05636},
	publisher = {{arXiv}},
	author = {Han, Qingyu and Yang, Linxin and Chen, Qian and Zhou, Xiang and Zhang, Dong and Wang, Akang and Sun, Ruoyu and Luo, Xiaodong},
	urldate = {2023-04-27},
	date = {2023-03-06},
	eprinttype = {arxiv},
	eprint = {2302.05636 [math]},
	keywords = {Mathematics - Optimization and Control, {GNN}, Pred-and-opt, imitation learning, end-to-end},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/97Y5HXCT/Han et al. - 2023 - A GNN-Guided Predict-and-Search Framework for Mixe.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/YUCISAPH/2302.html:text/html},
}

@misc{deza_machine_2023,
	title = {Machine Learning for Cutting Planes in Integer Programming: A Survey},
	url = {http://arxiv.org/abs/2302.09166},
	doi = {10.48550/arXiv.2302.09166},
	shorttitle = {Machine Learning for Cutting Planes in Integer Programming},
	abstract = {We survey recent work on machine learning ({ML}) techniques for selecting cutting planes (or cuts) in mixed-integer linear programming ({MILP}). Despite the availability of various classes of cuts, the task of choosing a set of cuts to add to the linear programming ({LP}) relaxation at a given node of the branch-and-bound (B\&B) tree has defied both formal and heuristic solutions to date. {ML} offers a promising approach for improving the cut selection process by using data to identify promising cuts that accelerate the solution of {MILP} instances. This paper presents an overview of the topic, highlighting recent advances in the literature, common approaches to data collection, evaluation, and {ML} model architectures. We analyze the empirical results in the literature in an attempt to quantify the progress that has been made and conclude by suggesting avenues for future research.},
	number = {{arXiv}:2302.09166},
	publisher = {{arXiv}},
	author = {Deza, Arnaud and Khalil, Elias B.},
	urldate = {2023-04-27},
	date = {2023-02-17},
	eprinttype = {arxiv},
	eprint = {2302.09166 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Computer Science - Artificial Intelligence, Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/AHWNUJBR/Deza and Khalil - 2023 - Machine Learning for Cutting Planes in Integer Pro.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/U8LEJWS6/2302.html:text/html},
}

@article{anderson_generative_2022,
	title = {Generative deep learning for decision making in gas networks},
	volume = {95},
	issn = {1432-5217},
	url = {https://doi.org/10.1007/s00186-022-00777-x},
	doi = {10.1007/s00186-022-00777-x},
	abstract = {A decision support system relies on frequent re-solving of similar problem instances. While the general structure remains the same in corresponding applications, the input parameters are updated on a regular basis. We propose a generative neural network design for learning integer decision variables of mixed-integer linear programming ({MILP}) formulations of these problems. We utilise a deep neural network discriminator and a {MILP} solver as our oracle to train our generative neural network. In this article, we present the results of our design applied to the transient gas optimisation problem. The trained generative neural network produces a feasible solution in 2.5s, and when used as a warm start solution, decreases global optimal solution time by 60.5\%.},
	pages = {503--532},
	number = {3},
	journaltitle = {Mathematical Methods of Operations Research},
	shortjournal = {Math Meth Oper Res},
	author = {Anderson, Lovis and Turner, Mark and Koch, Thorsten},
	urldate = {2023-04-27},
	date = {2022-06-01},
	langid = {english},
	keywords = {Deep learning, Mixed-integer programming, {SKIMMED}, Gas networks, Generative modelling, Primal heuristic, Pred-and-opt, imitation learning, end-to-end},
	file = {Full Text PDF:/home/brunompacheco/Zotero/storage/2WNLQ4NK/Anderson et al. - 2022 - Generative deep learning for decision making in ga.pdf:application/pdf},
}

@article{peng_heavy-head_2023,
	title = {Heavy-Head Sampling for Fast Imitation Learning of Machine Learning Based Combinatorial Auction Solver},
	volume = {55},
	issn = {1573-773X},
	url = {https://doi.org/10.1007/s11063-022-10900-y},
	doi = {10.1007/s11063-022-10900-y},
	abstract = {The winner determination problem of a combinatorial auction can be modeled as mixed-integer linear programming, and is a popular benchmark to evaluate modern solvers. Recent advancements in combinatorial optimization improve the branch-and-bound solving process by replacing the time-consuming heuristics with machine learning models. In this paper, by taking advantage of the heavy-head maximum depth distribution of the branch-and-bound solution trees, a heavy-head sampling strategy is proposed for the imitation learning on the combinatorial auction problems. Experimental results show that, under the small-dataset fast-training scheme and using the heavy-head sampling strategy, the final evaluation results of the trained policy on the combinatorial auction problems are improved significantly (in the sense of statistical testing), compared to using the uniform sampling strategy in previous studies.},
	pages = {631--644},
	number = {1},
	journaltitle = {Neural Processing Letters},
	shortjournal = {Neural Process Lett},
	author = {Peng, Chen and Liao, Bolin},
	urldate = {2023-04-27},
	date = {2023-02-01},
	langid = {english},
	keywords = {Combinatorial optimization, Combinatorial auction, Imitation learning, Neural network},
	file = {Full Text PDF:/home/brunompacheco/Zotero/storage/ENPUBKKJ/Peng and Liao - 2023 - Heavy-Head Sampling for Fast Imitation Learning of.pdf:application/pdf},
}

@misc{banitalebi-dehkordi_ml4co_2021,
	title = {{ML}4CO: Is {GCNN} All You Need? Graph Convolutional Neural Networks Produce Strong Baselines For Combinatorial Optimization Problems, If Tuned and Trained Properly, on Appropriate Data},
	url = {http://arxiv.org/abs/2112.12251},
	doi = {10.48550/arXiv.2112.12251},
	shorttitle = {{ML}4CO},
	abstract = {The 2021 {NeurIPS} Machine Learning for Combinatorial Optimization ({ML}4CO) competition was designed with the goal of improving state-of-the-art combinatorial optimization solvers by replacing key heuristic components with machine learning models. The competition's main scientific question was the following: is machine learning a viable option for improving traditional combinatorial optimization solvers on specific problem distributions, when historical data is available? This was motivated by the fact that in many practical scenarios, the data changes only slightly between the repetitions of a combinatorial optimization problem, and this is an area where machine learning models are particularly powerful at. This paper summarizes the solution and lessons learned by the Huawei {EI}-{OROAS} team in the dual task of the competition. The submission of our team achieved the second place in the final ranking, with a very close distance to the first spot. In addition, our solution was ranked first consistently for several weekly leaderboard updates before the final evaluation. We provide insights gained from a large number of experiments, and argue that a simple Graph Convolutional Neural Network ({GCNNs}) can achieve state-of-the-art results if trained and tuned properly.},
	number = {{arXiv}:2112.12251},
	publisher = {{arXiv}},
	author = {Banitalebi-Dehkordi, Amin and Zhang, Yong},
	urldate = {2023-04-27},
	date = {2021-12-22},
	eprinttype = {arxiv},
	eprint = {2112.12251 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Discrete Mathematics, {GNN}},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/YYQWCCFT/Banitalebi-Dehkordi and Zhang - 2021 - ML4CO Is GCNN All You Need Graph Convolutional N.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/SDCUEIBG/2112.html:text/html},
}

@article{khalil_mip-gnn_2022,
	title = {{MIP}-{GNN}: A Data-Driven Framework for Guiding Combinatorial Solvers},
	volume = {36},
	rights = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/21262},
	doi = {10.1609/aaai.v36i9.21262},
	shorttitle = {{MIP}-{GNN}},
	abstract = {Mixed-integer programming ({MIP}) technology offers a generic way of formulating and solving combinatorial optimization problems. While generally reliable, state-of-the-art {MIP} solvers base many crucial decisions on hand-crafted heuristics, largely ignoring common patterns within a given instance distribution of the problem of interest. Here, we propose {MIP}-{GNN}, a general framework for enhancing such solvers with data-driven insights. By encoding the variable-constraint interactions of a given mixed-integer linear program ({MILP}) as a bipartite graph, we leverage state-of-the-art graph neural network architectures to predict variable biases, i.e., component-wise averages of (near) optimal solutions, indicating how likely a variable will be set to 0 or 1 in (near) optimal solutions of binary {MILPs}. In turn, the predicted biases stemming from a single, once-trained model are used to guide the solver, replacing heuristic components. We integrate {MIP}-{GNN} into a state-of-the-art {MIP} solver, applying it to tasks such as node selection and warm-starting, showing significant improvements compared to the default setting of the solver on two classes of challenging binary {MILPs}. Our code and appendix are publicly available at https://github.com/lyeskhalil/{mipGNN}.},
	pages = {10219--10227},
	number = {9},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Khalil, Elias B. and Morris, Christopher and Lodi, Andrea},
	urldate = {2023-04-27},
	date = {2022-06-28},
	langid = {english},
	note = {Number: 9},
	keywords = {Machine Learning ({ML}), {GNN}, Pred-and-opt, *, imitation learning, end-to-end},
	file = {Full Text PDF:/home/brunompacheco/Zotero/storage/MYPXJW32/Khalil et al. - 2022 - MIP-GNN A Data-Driven Framework for Guiding Combi.pdf:application/pdf},
}

@inproceedings{mccarty_nn-baker_2021,
	title = {{NN}-Baker: A Neural-network Infused Algorithmic Framework for Optimization Problems on Geometric Intersection Graphs},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/c236337b043acf93c7df397fdb9082b3-Abstract.html},
	shorttitle = {{NN}-Baker},
	abstract = {Recent years have witnessed a surge of approaches to use neural networks to help tackle combinatorial optimization problems, including graph optimization problems. However, theoretical understanding of such approaches remains limited. In this paper, we consider the geometric setting, where graphs are induced by points in a fixed dimensional Euclidean space. We show that several graph optimization problems can be approximated by an algorithm that is polynomial in graph size n via a framework we propose, call the Baker-paradigm. More importantly, a key advantage of the Baker-paradigm is that it decomposes the input problem into (at most linear number of) small sub-problems of fixed sizes (independent of the size of the input). For the family of such fixed-size sub-problems, we can now design neural networks with universal approximation guarantees to solve them. This leads to a mixed algorithmic-{ML} framework, which we call {NN}-Baker that has the capacity to approximately solve a family of graph optimization problems (e.g, maximum independent set and minimum vertex cover) in time linear to input graph size, and only polynomial to approximation parameter. We instantiate our {NN}-Baker by a {CNN} version and {GNN} version, and demonstrate the effectiveness and efficiency of our approach via a range of experiments.},
	pages = {23023--23035},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {{McCarty}, Evan and Zhao, Qi and Sidiropoulos, Anastasios and Wang, Yusu},
	urldate = {2023-04-27},
	date = {2021},
	file = {Full Text PDF:/home/brunompacheco/Zotero/storage/9CDW65ZR/McCarty et al. - 2021 - NN-Baker A Neural-network Infused Algorithmic Fra.pdf:application/pdf},
}

@article{smith-miles_generating_2015,
	title = {Generating new test instances by evolving in instance space},
	volume = {63},
	issn = {0305-0548},
	url = {https://www.sciencedirect.com/science/article/pii/S0305054815001136},
	doi = {10.1016/j.cor.2015.04.022},
	abstract = {Our confidence in the future performance of any algorithm, including optimization algorithms, depends on how carefully we select test instances so that the generalization of algorithm performance on future instances can be inferred. In recent work, we have established a methodology to generate a 2-d representation of the instance space, comprising a set of known test instances. This instance space shows the similarities and differences between the instances using measurable features or properties, and enables the performance of algorithms to be viewed across the instance space, where generalizations can be inferred. The power of this methodology is the insights that can be generated into algorithm strengths and weaknesses by examining the regions in instance space where strong performance can be expected. The representation of the instance space is dependent on the choice of test instances however. In this paper we present a methodology for generating new test instances with controllable properties, by filling observed gaps in the instance space. This enables the generation of rich new sets of test instances to support better the understanding of algorithm strengths and weaknesses. The methodology is demonstrated on graph colouring as a case study.},
	pages = {102--113},
	journaltitle = {Computers \& Operations Research},
	shortjournal = {Computers \& Operations Research},
	author = {Smith-Miles, Kate and Bowly, Simon},
	urldate = {2023-04-27},
	date = {2015-11-01},
	langid = {english},
	keywords = {Benchmarking, Evolving instances, Graph colouring, Instance space, Test instances},
	file = {ScienceDirect Full Text PDF:/home/brunompacheco/Zotero/storage/TUJIQTSJ/Smith-Miles and Bowly - 2015 - Generating new test instances by evolving in insta.pdf:application/pdf;ScienceDirect Snapshot:/home/brunompacheco/Zotero/storage/5XNWF4QM/S0305054815001136.html:text/html},
}

@article{srinivasan_fast_2021,
	title = {Fast Multi-Robot Motion Planning via Imitation Learning of Mixed-Integer Programs},
	volume = {54},
	issn = {2405-8963},
	url = {https://www.sciencedirect.com/science/article/pii/S2405896321022813},
	doi = {10.1016/j.ifacol.2021.11.237},
	series = {Modeling, Estimation and Control Conference {MECC} 2021},
	abstract = {We propose a centralized multi-robot motion planning approach that leverages machine learning and mixed-integer programming ({MIP}). We train a neural network to imitate optimal {MIP} solutions and, during execution, the trajectories predicted by the network are used to fix most of the integer variables, resulting in a significantly reduced {MIP} or even a convex program. If the obtained trajectories are feasible, i.e., collision-free and reaching the goal, they can be used as they are or further refined towards optimality. Since maximizing the likelihood of feasibility is not the standard goal of imitation learning, we propose several techniques aimed at increasing such likelihood. Simulation results show the reduced computational burden associated with the proposed framework and the similarity with the optimal {MIP} solutions.},
	pages = {598--604},
	number = {20},
	journaltitle = {{IFAC}-{PapersOnLine}},
	shortjournal = {{IFAC}-{PapersOnLine}},
	author = {Srinivasan, Mohit and Chakrabarty, Ankush and Quirynen, Rien and Yoshikawa, Nobuyuki and Mariyama, Toshisada and Cairano, Stefano Di},
	urldate = {2023-04-27},
	date = {2021-01-01},
	langid = {english},
	keywords = {Machine Learning, Motion Control, Optimal Control, Path Planning, Pred-and-opt, imitation learning, end-to-end},
	file = {ScienceDirect Full Text PDF:/home/brunompacheco/Zotero/storage/68Y93593/Srinivasan et al. - 2021 - Fast Multi-Robot Motion Planning via Imitation Lea.pdf:application/pdf},
}

@article{wang_acceleration_2023,
	title = {Acceleration Framework and Solution Algorithm for Distribution System Restoration based on End-to-End Optimization Strategy},
	issn = {1558-0679},
	doi = {10.1109/TPWRS.2023.3262189},
	abstract = {The distribution system restoration ({DSR}) problem is traditionally modeled as a mixed-integer linear programming ({MILP}) model. However, a significant number of integer variables are introduced to describe the {DSR} process, which introduce additional complexities in both time and space dimensions. Moreover, an enormous number of constraints are constructed to establish a rational {DSR} decision, while some of them may not be considered tight in practice. The enormous number of binary variables and inactive constraints could make the {DSR} problem very hard to solve and apply in real-time. The {DSR} computation burden would be reduced significantly if binary variables and binding constraints are pre-determined. This paper proposes an acceleration framework and solution algorithm based on the end-to-end optimization, which applies deep neural network ({DNN}) and gradient boosting decision tree ({GBDT}) methods to {DSR}. The {DSR} problem, which is solved in offline and online stages, will accordingly be reduced to a linear programming problem which can be solved more efficiently and reliably. Case studies are carried out on the modified {IEEE} 33-bus and 123-bus systems and a practical 1069-bus system. The proposed results indicate that the {DSR} problem with the proposed end-to-end acceleration framework is solved more than tenfold faster than those of traditional solvers.},
	pages = {1--13},
	journaltitle = {{IEEE} Transactions on Power Systems},
	author = {Wang, Yifei and Yan, Ziheng and Sang, Linwei and Hong, Lucheng and Hu, Qinran and Shahidehpour, Mohammad and Xu, Qingshan},
	date = {2023},
	note = {Conference Name: {IEEE} Transactions on Power Systems},
	keywords = {Computational modeling, Optimization, acceleration algorithm, Decision trees, deep neutral network, Distribution system restoration, Generators, gradient boosting decision tree, Indexes, Network topology, Voltage},
	file = {IEEE Xplore Abstract Record:/home/brunompacheco/Zotero/storage/QE7Z8JD6/10081430.html:text/html},
}

@misc{sun_enhancing_2023,
	title = {Enhancing Constraint Programming via Supervised Learning for Job Shop Scheduling},
	url = {http://arxiv.org/abs/2211.14492},
	doi = {10.48550/arXiv.2211.14492},
	abstract = {Constraint programming ({CP}) is a powerful technique for solving constraint satisfaction and optimization problems. In {CP} solvers, the variable ordering strategy used to select which variable to explore first in the solving process has a significant impact on solver effectiveness. To address this issue, we propose a novel variable ordering strategy based on supervised learning, which we evaluate in the context of job shop scheduling problems. Our learning-based methods predict the optimal solution of a problem instance and use the predicted solution to order variables for {CP} solvers. {\textbackslash}added[]\{Unlike traditional variable ordering methods, our methods can learn from the characteristics of each problem instance and customize the variable ordering strategy accordingly, leading to improved solver performance.\} Our experiments demonstrate that training machine learning models is highly efficient and can achieve high accuracy. Furthermore, our learned variable ordering methods perform competitively when compared to four existing methods. Finally, we demonstrate that hybridising the machine learning-based variable ordering methods with traditional domain-based methods is beneficial.},
	number = {{arXiv}:2211.14492},
	publisher = {{arXiv}},
	author = {Sun, Yuan and Nguyen, Su and Thiruvady, Dhananjay and Li, Xiaodong and Ernst, Andreas T. and Aickelin, Uwe},
	urldate = {2023-04-27},
	date = {2023-04-12},
	eprinttype = {arxiv},
	eprint = {2211.14492 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/WPH22NRV/Sun et al. - 2023 - Enhancing Constraint Programming via Supervised Le.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/PGHU4U9K/2211.html:text/html;Full Text:/home/brunompacheco/Zotero/storage/3ALYE5SL/Sun et al. - 2023 - Enhancing Constraint Programming via Supervised Le.pdf:application/pdf},
}

@misc{dalle_learning_2022,
	title = {Learning with Combinatorial Optimization Layers: a Probabilistic Approach},
	url = {http://arxiv.org/abs/2207.13513},
	doi = {10.48550/arXiv.2207.13513},
	shorttitle = {Learning with Combinatorial Optimization Layers},
	abstract = {Combinatorial optimization ({CO}) layers in machine learning ({ML}) pipelines are a powerful tool to tackle data-driven decision tasks, but they come with two main challenges. First, the solution of a {CO} problem often behaves as a piecewise constant function of its objective parameters. Given that {ML} pipelines are typically trained using stochastic gradient descent, the absence of slope information is very detrimental. Second, standard {ML} losses do not work well in combinatorial settings. A growing body of research addresses these challenges through diverse methods. Unfortunately, the lack of well-maintained implementations slows down the adoption of {CO} layers. In this paper, building upon previous works, we introduce a probabilistic perspective on {CO} layers, which lends itself naturally to approximate differentiation and the construction of structured losses. We recover many approaches from the literature as special cases, and we also derive new ones. Based on this unifying perspective, we present {InferOpt}.jl, an open-source Julia package that 1) allows turning any {CO} oracle with a linear objective into a differentiable layer, and 2) defines adequate losses to train pipelines containing such layers. Our library works with arbitrary optimization algorithms, and it is fully compatible with Julia's {ML} ecosystem. We demonstrate its abilities using a pathfinding problem on video game maps as guiding example, as well as three other applications from operations research.},
	number = {{arXiv}:2207.13513},
	publisher = {{arXiv}},
	author = {Dalle, Guillaume and Baty, Léo and Bouvier, Louis and Parmentier, Axel},
	urldate = {2023-04-27},
	date = {2022-12-03},
	eprinttype = {arxiv},
	eprint = {2207.13513 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning, optimization layers},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/HTL86ZZG/Dalle et al. - 2022 - Learning with Combinatorial Optimization Layers a.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/47GMUXFA/2207.html:text/html},
}

@article{zhang_survey_2023,
	title = {A survey for solving mixed integer programming via machine learning},
	volume = {519},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231222014035},
	doi = {10.1016/j.neucom.2022.11.024},
	abstract = {Machine learning ({ML}) has been recently introduced to solving optimization problems, especially for combinatorial optimization ({CO}) tasks. In this paper, we survey the trend of leveraging {ML} to solve the mixed-integer programming problem ({MIP}). Theoretically, {MIP} is an {NP}-hard problem, and most {CO} problems can be formulated as {MIP}. Like other {CO} problems, the human-designed heuristic algorithms for {MIP} rely on good initial solutions and cost a lot of computational resources. Therefore, researchers consider applying machine learning methods to solve {MIP} since {ML}-enhanced approaches can provide the solution based on the typical patterns from the training data. Specifically, we first introduce the formulation and preliminaries of {MIP} and representative traditional solvers. Then, we show the integration of machine learning and {MIP} with detailed discussions on related learning-based methods, which can be further classified into exact and heuristic algorithms. Finally, we propose the outlook for learning-based {MIP} solvers, the direction toward more combinatorial optimization problems beyond {MIP}, and the mutual embrace of traditional solvers and {ML} components. We maintain a list of papers that utilize machine learning technologies to solve combinatorial optimization problems, which is available at https://github.com/Thinklab-{SJTU}/awesome-ml4co.},
	pages = {205--217},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Zhang, Jiayi and Liu, Chang and Li, Xijun and Zhen, Hui-Ling and Yuan, Mingxuan and Li, Yawen and Yan, Junchi},
	urldate = {2023-04-27},
	date = {2023-01-28},
	langid = {english},
	keywords = {Machine learning, Combinatorial optimization, Mixed integer programming, {SURVEY}},
	file = {ScienceDirect Full Text PDF:/home/brunompacheco/Zotero/storage/FZ94QZLH/Zhang et al. - 2023 - A survey for solving mixed integer programming via.pdf:application/pdf},
}

@misc{borozan_machine_2023,
	title = {A Machine Learning-Enhanced Benders Decomposition Approach to Solve the Transmission Expansion Planning Problem under Uncertainty},
	url = {http://arxiv.org/abs/2304.07534},
	doi = {10.48550/arXiv.2304.07534},
	abstract = {The necessary decarbonization efforts in energy sectors entail the integration of flexibility assets, as well as increased levels of uncertainty for the planning and operation of power systems. To cope with this in a cost-effective manner, transmission expansion planning ({TEP}) models need to incorporate progressively more details to represent potential long-term system developments and the operation of power grids with intermittent renewable generation. However, the increased modeling complexities of {TEP} exercises can easily lead to computationally intractable optimization problems. Currently, most techniques that address computational intractability alter the original problem, thus neglecting critical modeling aspects or affecting the structure of the optimal solution. In this paper, we propose an alternative approach to significantly alleviate the computational burden of large-scale {TEP} problems. Our approach integrates machine learning ({ML}) with the well-established Benders decomposition to manage the problem size while preserving solution quality. The proposed {ML}-enhanced Multicut Benders Decomposition algorithm improves computational efficiency by identifying effective and ineffective optimality cuts via supervised learning techniques. We illustrate the benefits of the proposed methodology by solving a number of multi-stage {TEP} problems of different sizes, based on the {IEEE}24 and {IEEE}118 test systems, while also considering energy storage investment options.},
	number = {{arXiv}:2304.07534},
	publisher = {{arXiv}},
	author = {Borozan, Stefan and Giannelos, Spyros and Falugi, Paola and Moreira, Alexandre and Strbac, Goran},
	urldate = {2023-04-27},
	date = {2023-04-15},
	eprinttype = {arxiv},
	eprint = {2304.07534 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control, imitation learning},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/FRD9KJGQ/Borozan et al. - 2023 - A Machine Learning-Enhanced Benders Decomposition .pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/EB7FWCJT/2304.html:text/html},
}

@misc{ferber_surco_2022,
	title = {{SurCo}: Learning Linear Surrogates For Combinatorial Nonlinear Optimization Problems},
	url = {http://arxiv.org/abs/2210.12547},
	doi = {10.48550/arXiv.2210.12547},
	shorttitle = {{SurCo}},
	abstract = {Optimization problems with expensive nonlinear cost functions and combinatorial constraints appear in many real-world applications, but remain challenging to solve efficiently. Existing combinatorial solvers like Mixed Integer Linear Programming can be fast in practice but cannot readily optimize nonlinear cost functions, while general nonlinear optimizers like gradient descent often do not handle complex combinatorial structures, may require many queries of the cost function, and are prone to local optima. To bridge this gap, we propose {SurCo} that learns linear Surrogate costs which can be used by existing Combinatorial solvers to output good solutions to the original nonlinear combinatorial optimization problem, combining the flexibility of gradient-based methods with the structure of linear combinatorial optimization. We learn these linear surrogates end-to-end with the nonlinear loss by differentiating through the linear surrogate solver. Three variants of {SurCo} are proposed: {SurCo}-zero operates on individual nonlinear problems, {SurCo}-prior trains a linear surrogate predictor on distributions of problems, and {SurCo}-hybrid uses a model trained offline to warm start online solving for {SurCo}-zero. We analyze our method theoretically and empirically, showing smooth convergence and improved performance. Experiments show that compared to state-of-the-art approaches and expert-designed heuristics, {SurCo} obtains lower cost solutions with comparable or faster solve time for two realworld industry-level applications: embedding table sharding and inverse photonic design.},
	number = {{arXiv}:2210.12547},
	publisher = {{arXiv}},
	author = {Ferber, Aaron and Huang, Taoan and Zha, Daochen and Schubert, Martin and Steiner, Benoit and Dilkina, Bistra and Tian, Yuandong},
	urldate = {2023-04-27},
	date = {2022-10-22},
	eprinttype = {arxiv},
	eprint = {2210.12547 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/C3H8K8N5/Ferber et al. - 2022 - SurCo Learning Linear Surrogates For Combinatoria.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/6VJCGAVX/2210.html:text/html},
}

@article{xiao_towards_2022,
	title = {Towards a machine learning-aided metaheuristic framework for a production/distribution system design problem},
	volume = {146},
	issn = {0305-0548},
	url = {https://www.sciencedirect.com/science/article/pii/S0305054822001605},
	doi = {10.1016/j.cor.2022.105897},
	abstract = {Recent advances have seen vast success in the application of metaheuristics in {NP}-hard combinatorial problems. A generic metaheuristic design usually consists of three core elements that jointly determine the algorithm performance, including an initial candidate solution, a guided search procedure, and a fitness function that approximates the objective value. This paper proposes a data-driven metaheuristic ({DDMH}) framework that leverages the predictive power of machine learning models, which exploit location information and mine structural knowledge of a supply chain network for intelligent decision making. Specifically, the proposed framework offers three performance boosters, including an initial solution heuristic, a narrowed search space, and an efficient learning-based fitness function. The framework can be readily integrated into existing {MHs}. As a case study, we apply {DDMH} to a production/distribution network design problem. Experimental results show that the {DDMH} outperforms the traditional {MHs} with better solution quality and comparable running time, especially for hard problems.},
	pages = {105897},
	journaltitle = {Computers \& Operations Research},
	shortjournal = {Computers \& Operations Research},
	author = {Xiao, Zhifeng and Zhi, Jianing and Keskin, Burcu B.},
	urldate = {2023-04-27},
	date = {2022-10-01},
	langid = {english},
	keywords = {Machine learning, Fitness function, Initial solution, Metaheuristics, Network design, Search space, Pred-and-opt, end-to-end},
	file = {ScienceDirect Full Text PDF:/home/brunompacheco/Zotero/storage/9KF964MI/Xiao et al. - 2022 - Towards a machine learning-aided metaheuristic fra.pdf:application/pdf;ScienceDirect Snapshot:/home/brunompacheco/Zotero/storage/S423PI24/S0305054822001605.html:text/html},
}

@article{schweitzer_choosing_2023,
	title = {Choosing Solution Strategies for Scheduling Automated Guided Vehicles in Production Using Machine Learning},
	volume = {13},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/13/2/806},
	doi = {10.3390/app13020806},
	abstract = {Artificial intelligence is considered to be a significant technology for driving the future evolution of smart manufacturing environments. At the same time, automated guided vehicles ({AGVs}) play an essential role in manufacturing systems due to their potential to improve internal logistics by increasing production flexibility. Thereby, the productivity of the entire system relies on the quality of the schedule, which can achieve production cost savings by minimizing delays and the total makespan. However, traditional scheduling algorithms often have difficulties in adapting to changing environment conditions, and the performance of a selected algorithm depends on the individual scheduling problem. Therefore, this paper aimed to analyze the scheduling problem classes of {AGVs} by applying design science research to develop an algorithm selection approach. The designed artifact addressed a catalogue of characteristics that used several machine learning algorithms to find the optimal solution strategy for the intended scheduling problem. The contribution of this paper is the creation of an algorithm selection method that automatically selects a scheduling algorithm, depending on the problem class and the algorithm space. In this way, production efficiency can be increased by dynamically adapting the {AGV} schedules. A computational study with benchmark literature instances unveiled the successful implementation of constraint programming solvers for solving {JSSP} and {FJSSP} scheduling problems and machine learning algorithms for predicting the most promising solver. The performance of the solvers strongly depended on the given problem class and the problem instance. Consequently, the overall production performance increased by selecting the algorithms per instance. A field experiment in the learning factory at Reutlingen University enabled the validation of the approach within a running production scenario.},
	pages = {806},
	number = {2},
	journaltitle = {Applied Sciences},
	author = {Schweitzer, Felicia and Bitsch, Günter and Louw, Louis},
	urldate = {2023-04-27},
	date = {2023-01},
	langid = {english},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {machine learning, optimization, {AGV} scheduling, algorithm selection, constraint programming},
	file = {Full Text PDF:/home/brunompacheco/Zotero/storage/BG7B95BG/Schweitzer et al. - 2023 - Choosing Solution Strategies for Scheduling Automa.pdf:application/pdf},
}

@online{noauthor_fast_nodate,
	title = {‪Fast and Robust Resource-Constrained Scheduling with Graph Neural Networks‬},
	url = {https://scholar.google.com/citations?view_op=view_citation&hl=en&user=avOLwlYAAAAJ&sortby=pubdate&citation_for_view=avOLwlYAAAAJ:4OULZ7Gr8RgC},
	abstract = {‪F Teichteil-Königsbuch, G Povéda, {GG} de Garibay Barba, T Luchterhand, S Thiébaux, 2023‬},
	urldate = {2023-04-27},
	file = {Snapshot:/home/brunompacheco/Zotero/storage/7BSYDQT5/citations.html:text/html},
}

@misc{benidis_solving_2023,
	title = {Solving Recurrent {MIPs} with Semi-supervised Graph Neural Networks},
	url = {http://arxiv.org/abs/2302.11992},
	doi = {10.48550/arXiv.2302.11992},
	abstract = {We propose an {ML}-based model that automates and expedites the solution of {MIPs} by predicting the values of variables. Our approach is motivated by the observation that many problem instances share salient features and solution structures since they differ only in few (time-varying) parameters. Examples include transportation and routing problems where decisions need to be re-optimized whenever commodity volumes or link costs change. Our method is the first to exploit the sequential nature of the instances being solved periodically, and can be trained with ``unlabeled'' instances, when exact solutions are unavailable, in a semi-supervised setting. Also, we provide a principled way of transforming the probabilistic predictions into integral solutions. Using a battery of experiments with representative binary {MIPs}, we show the gains of our model over other {ML}-based optimization approaches.},
	number = {{arXiv}:2302.11992},
	publisher = {{arXiv}},
	author = {Benidis, Konstantinos and Rosolia, Ugo and Rangapuram, Syama and Iosifidis, George and Paschos, Georgios},
	urldate = {2023-04-28},
	date = {2023-02-20},
	eprinttype = {arxiv},
	eprint = {2302.11992 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning, {GNN}, imitation learning, end-to-end},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/CXZVYLI8/Benidis et al. - 2023 - Solving Recurrent MIPs with Semi-supervised Graph .pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/R6MITMS5/2302.html:text/html},
}

@thesis{zarpellon_machine_2020,
	location = {Canada -- Quebec, {CA}},
	title = {Machine Learning Algorithms in Mixed-Integer Programming},
	rights = {Database copyright {ProQuest} {LLC}; {ProQuest} does not claim copyright in the individual underlying works.},
	url = {https://www.proquest.com/docview/2626025254/abstract/C7AB98C4D5F64E0APQ/1},
	abstract = {A variety of real-world tasks involve decisions of discrete nature, and can be mathematically modeled as Mixed-Integer Programming ({MIP}) optimization problems. Daily deployed in multiple application domains, {MIP} formulations are nowadays solved effectively and reliably, thanks to the complex and rich algorithmic frameworks developed in modern {MIP} solvers. With machine learning ({ML}) being extensively leveraged to “learn from examples”, new attention has been given to the application of learned predictions in optimization settings, especially in the context of {MIP} solvers’ algorithmic design. The present thesis contributes to this thread of research: we discuss and propose novel methods on the theme of using {ML} algorithms alongside {MIP} ones, and explore some opportunities of this fruitful interaction.
First, to document {ML} attempts addressing the decisions of variable and node selection in Branch and Bound (B\&B), we present a survey on learning and branching. By interpreting previous {MIP} literature contributions as forerunners of {ML}-based works, and discussing the assumptions and concerns underlying these critical heuristic decisions, we provide an original canvas to analyze recent learned approaches and outline new points of view. Second, we develop a classification framework to tackle the algorithmic question of whether to linearize convex quadratic {MIPs}, aiming at a tight integration of the optimization knowledge in the learning pipeline. As experiments practically led to the deployment of a classifier in the {IBM}-{CPLEX} optimization solver, the work more generally outlines a reference methodology for the combination of {ML} and {MIP} technologies.
Alternate abstract:
De nombreux problèmes de la vie courante comportent des décisions discrètes, et peuvent être modélisés sous la forme de programmes d’optimisation en nombres entiers. De tels modèles peuvent désormais être résolus efficacement à l’aide de solveurs matures comportant un vaste arsenal algorithmique, ce qui explique l’utilisation quotidienne de la programmation mathématique mixte en nombres entiers ({PNE}) dans de multiples secteurs. Alors que les techniques d’apprentissage automatiques (à partir d’exemples) sont de plus en plus mises en œuvre pour l’analyse de données et la predicion, une attention nouvelle est donnée à l’apprentissage dans le contexte d’algorithmes d’optimisation, notamment dans les choix algorithmiques des solveurs de {PNE}. Cette thèse s’inscrit dans ce courant de recherche : nous analysons et proposons de nouvelles méthodes pour intégrer des techniques d’apprentissage automatique au sein d’algorithmes d’optimisation, et explorons le potentiel de cette interaction. Premièrement, nous passons en revue l’état de l’art quant à l’utilisation de techniques d’apprentissage automatique pour la sélection de variables et de nœuds dans un arbre de branchement. Plusieurs travaux de {PNE} sont identifiés comme précurseurs d’approches basées sur l’apprentissage automatique. En discutant les hypothèses et questions sousjacentes de ces décisions, nous proposons un nouveau cadre pour analyser les approches récentes d’apprentissage, et soulignons des nouvelles perspectives quant à l’utilisation de l’apprentissage pour guider le branchement.},
	pagetotal = {157},
	institution = {Ecole Polytechnique, Montreal (Canada)},
	type = {phdthesis},
	author = {Zarpellon, Giulia},
	urldate = {2023-04-28},
	date = {2020},
	note = {{ISBN}: 9798759977353},
	keywords = {Algorithms, Integer programming, Computer science, Artificial intelligence, Machine learning, Applied mathematics, Neural networks, Optimization, Datasets, Decision making, Distance learning, Feature selection, Linear programming},
	file = {Full Text PDF:/home/brunompacheco/Zotero/storage/98IMC4AW/Zarpellon - 2020 - Machine Learning Algorithms in Mixed-Integer Progr.pdf:application/pdf},
}

@misc{kannan_learning_2023,
	title = {Learning to Accelerate the Global Optimization of Quadratically-Constrained Quadratic Programs},
	url = {http://arxiv.org/abs/2301.00306},
	doi = {10.48550/arXiv.2301.00306},
	abstract = {We learn optimal instance-specific heuristics for the global minimization of nonconvex quadratically-constrained quadratic programs ({QCQPs}). Specifically, we consider partitioning-based mixed-integer programming relaxations for nonconvex {QCQPs} and propose the novel problem of \${\textbackslash}textit\{strong partitioning\}\$ to optimally partition variable domains \${\textbackslash}textit\{without\}\$ sacrificing global optimality. We design a local optimization method for solving this challenging max-min strong partitioning problem and replace this expensive benchmark strategy with a machine learning ({ML}) approximation for homogeneous families of {QCQPs}. We present a detailed computational study on randomly generated families of {QCQPs}, including instances of the pooling problem, using the open-source global solver Alpine. Our numerical experiments demonstrate that strong partitioning and its {ML} approximation significantly reduce Alpine's solution time by factors of \$3.5 - 16.5\$ and \$2 - 4.5\$ \${\textbackslash}textit\{on average\}\$ and by maximum factors of \$15 - 700\$ and \$10 - 200\$, respectively, over the different {QCQP} families.},
	number = {{arXiv}:2301.00306},
	publisher = {{arXiv}},
	author = {Kannan, Rohit and Nagarajan, Harsha and Deka, Deepjyoti},
	urldate = {2023-04-28},
	date = {2023-02-22},
	eprinttype = {arxiv},
	eprint = {2301.00306 [math]},
	keywords = {Mathematics - Optimization and Control, 90C26, 90C31, 65K05},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/2XS6YPSB/Kannan et al. - 2023 - Learning to Accelerate the Global Optimization of .pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/RNCY4ICT/2301.html:text/html},
}

@misc{duan_augment_2022,
	title = {Augment with Care: Contrastive Learning for Combinatorial Problems},
	url = {http://arxiv.org/abs/2202.08396},
	doi = {10.48550/arXiv.2202.08396},
	shorttitle = {Augment with Care},
	abstract = {Supervised learning can improve the design of state-of-the-art solvers for combinatorial problems, but labelling large numbers of combinatorial instances is often impractical due to exponential worst-case complexity. Inspired by the recent success of contrastive pre-training for images, we conduct a scientific study of the effect of augmentation design on contrastive pre-training for the Boolean satisfiability problem. While typical graph contrastive pre-training uses label-agnostic augmentations, our key insight is that many combinatorial problems have well-studied invariances, which allow for the design of label-preserving augmentations. We find that label-preserving augmentations are critical for the success of contrastive pre-training. We show that our representations are able to achieve comparable test accuracy to fully-supervised learning while using only 1\% of the labels. We also demonstrate that our representations are more transferable to larger problems from unseen domains. Our code is available at https://github.com/h4duan/contrastive-sat.},
	number = {{arXiv}:2202.08396},
	publisher = {{arXiv}},
	author = {Duan, Haonan and Vaezipoor, Pashootan and Paulus, Max B. and Ruan, Yangjun and Maddison, Chris J.},
	urldate = {2023-04-28},
	date = {2022-06-20},
	eprinttype = {arxiv},
	eprint = {2202.08396 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, {GNN}, *, imitation learning, background, end-to-end},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/LVGN7G87/Duan et al. - 2022 - Augment with Care Contrastive Learning for Combin.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/JTT7KFGW/2202.html:text/html},
}

@misc{sugishita_warmstarting_2021,
	title = {Warmstarting column generation for unit commitment},
	url = {http://arxiv.org/abs/2110.06872},
	doi = {10.48550/arXiv.2110.06872},
	abstract = {This work focuses on a solution method for unit commitment problems which are to be solved repeatedly with different data but with the same problem structures. Dantzig-Wolfe decomposition with a column generation procedure has been proved successful for the unit commitment problem. We demonstrate use of a neural network to generate initial dual values for the column generation procedure, and show how the neural network can be trained efficiently using dual decomposition. Our numerical experiments compare our methods with baselines and reveal that our approach solves test instances to high precision in shorter computational time and scales to handle larger-scale instances.},
	number = {{arXiv}:2110.06872},
	publisher = {{arXiv}},
	author = {Sugishita, Nagisa and Grothey, Andreas and {McKinnon}, Ken},
	urldate = {2023-04-28},
	date = {2021-10-13},
	eprinttype = {arxiv},
	eprint = {2110.06872 [math]},
	keywords = {Mathematics - Optimization and Control, imitation learning, end-to-end},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/4W23Q585/Sugishita et al. - 2021 - Warmstarting column generation for unit commitment.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/KVQ8YZ8C/2110.html:text/html},
}

@article{bongiovanni_machine_2022,
	title = {A machine learning-driven two-phase metaheuristic for autonomous ridesharing operations},
	volume = {165},
	issn = {1366-5545},
	url = {https://www.sciencedirect.com/science/article/pii/S1366554522002198},
	doi = {10.1016/j.tre.2022.102835},
	abstract = {This paper contributes to the intersection of operations research and machine learning in the context of autonomous ridesharing. In this work, autonomous ridesharing operations are reproduced through an event-based simulation approach and are modeled as a sequence of static subproblems to be optimized. The optimization framework consists of a novel data-driven metaheuristic within a two phase approach. The first phase consists of a greedy insertion heuristic that assigns new online requests to vehicles. The second phase consists of a local-search based metaheuristic that iteratively revisits previously-made vehicle-trip assignments through intra- and inter-vehicle route exchanges. These exchanges are performed by selecting from a pool of destroy–repair operators using a machine learning approach that is trained offline on a large dataset composed of more than one and a half million examples of previously-solved autonomous ridesharing subproblems. Computational results are performed on multiple dynamic instances extracted from real ridesharing data published by Uber Technologies Inc. Results show that the proposed machine learning-based optimization approach outperforms benchmark state-of-the-art data-driven metaheuristics by up to about nine percent, on average. Managerial insights highlight the correlation between selected vehicle routing features and the performance of the metaheuristics in the context of autonomous ridesharing operations.},
	pages = {102835},
	journaltitle = {Transportation Research Part E: Logistics and Transportation Review},
	shortjournal = {Transportation Research Part E: Logistics and Transportation Review},
	author = {Bongiovanni, Claudia and Kaspi, Mor and Cordeau, Jean-François and Geroliminis, Nikolas},
	urldate = {2023-04-28},
	date = {2022-09-01},
	langid = {english},
	keywords = {Machine learning, Metaheuristics, Dial-a-ride problem, Electric autonomous vehicles, Large neighborhood search, Online optimization},
	file = {ScienceDirect Full Text PDF:/home/brunompacheco/Zotero/storage/RGNR55F2/Bongiovanni et al. - 2022 - A machine learning-driven two-phase metaheuristic .pdf:application/pdf},
}

@article{peng_graph_2021,
	title = {Graph Learning for Combinatorial Optimization: A Survey of State-of-the-Art},
	volume = {6},
	issn = {2364-1541},
	url = {https://doi.org/10.1007/s41019-021-00155-3},
	doi = {10.1007/s41019-021-00155-3},
	shorttitle = {Graph Learning for Combinatorial Optimization},
	abstract = {Graphs have been widely used to represent complex data in many applications, such as e-commerce, social networks, and bioinformatics. Efficient and effective analysis of graph data is important for graph-based applications. However, most graph analysis tasks are combinatorial optimization ({CO}) problems, which are {NP}-hard. Recent studies have focused a lot on the potential of using machine learning ({ML}) to solve graph-based {CO} problems. Most recent methods follow the two-stage framework. The first stage is graph representation learning, which embeds the graphs into low-dimension vectors. The second stage uses machine learning to solve the {CO} problems using the embeddings of the graphs learned in the first stage. The works for the first stage can be classified into two categories, graph embedding methods and end-to-end learning methods. For graph embedding methods, the learning of the the embeddings of the graphs has its own objective, which may not rely on the {CO} problems to be solved. The {CO} problems are solved by independent downstream tasks. For end-to-end learning methods, the learning of the embeddings of the graphs does not have its own objective and is an intermediate step of the learning procedure of solving the {CO} problems. The works for the second stage can also be classified into two categories, non-autoregressive methods and autoregressive methods. Non-autoregressive methods predict a solution for a {CO} problem in one shot. A non-autoregressive method predicts a matrix that denotes the probability of each node/edge being a part of a solution of the {CO} problem. The solution can be computed from the matrix using search heuristics such as beam search. Autoregressive methods iteratively extend a partial solution step by step. At each step, an autoregressive method predicts a node/edge conditioned to current partial solution, which is used to its extension. In this survey, we provide a thorough overview of recent studies of the graph learning-based {CO} methods. The survey ends with several remarks on future research directions.},
	pages = {119--141},
	number = {2},
	journaltitle = {Data Science and Engineering},
	shortjournal = {Data Sci. Eng.},
	author = {Peng, Yun and Choi, Byron and Xu, Jianliang},
	urldate = {2023-04-28},
	date = {2021-06-01},
	langid = {english},
	keywords = {Combinational optimization, Graph neural network, Graph representation learning, {SURVEY}, {GNN}},
	file = {Full Text PDF:/home/brunompacheco/Zotero/storage/RQ3QW44A/Peng et al. - 2021 - Graph Learning for Combinatorial Optimization A S.pdf:application/pdf},
}

@thesis{gonzalez_jurado_machine_2020,
	location = {Canada -- Quebec, {CA}},
	title = {Machine Learning-Driven Hybrid Optimization Based on Decision Diagrams},
	rights = {Database copyright {ProQuest} {LLC}; {ProQuest} does not claim copyright in the individual underlying works.},
	url = {https://www.proquest.com/docview/2626891456/abstract/2CC07595476D41B1PQ/1},
	abstract = {The discrete and finite nature of combinatorial optimization problems arises in many areas of mathematics and computer science as well as in applications such as scheduling and planning. Despite decades of development and remarkable speedups in general-purpose solvers, some combinatorial problems are still difficult to be solved. The design of generic optimization solvers to tackle such challenging problems is a continuous and active research area. In this dissertation, we propose novel hybrid optimization mechanisms that exploit complementary strengths from different paradigms. The integrated mechanisms leverage (i) the decision diagram-based optimization ({DDO}) solving approach, (ii) a more mature technology such as mixed-integer programming ({MIP}), and, finally, (iii) the use of machine learning ({ML}) to enhance optimization methods.
Alternate abstract:
Les problèmes d’optimisation combinatoire se posent dans de nombreux domaines des mathématiques et de l’informatique, ainsi que dans des applications telles que l’ordonnancement et la planification. Malgré des décennies de développement des différentes technologies d’optimisation, certains problèmes combinatoires restent encore difficiles à résoudre. Le développement d’outils d’optimisation génériques pour résoudre ces problèmes difficiles est donc un domaine de recherche actif et continu. Dans cette thèse, nous proposons de nouveaux mécanismes d’optimisation hybrides qui exploitent les avantages complémentaires de différents paradigmes, à savoir, (i) l’optimisation basée sur les diagrammes de décision ({ODD}), (ii) la programmation en nombres entiers ({PNE}), et (iii) l’apprentissage automatique ({AA}) pour améliorer les méthodes d’optimisation.},
	pagetotal = {126},
	institution = {Ecole Polytechnique, Montreal (Canada)},
	type = {phdthesis},
	author = {Gonzalez Jurado, Jaime Esteban},
	urldate = {2023-04-28},
	date = {2020},
	note = {{ISBN}: 9798759978978},
	keywords = {Algorithms, Integer programming, Computer science, Artificial intelligence, Machine learning, Applied mathematics, Decision trees, Decision making, Linear programming, Automation, Dynamic programming, Mathematical programming, Semidefinite programming, Traveling salesman problem},
}

@thesis{sylvestre-decary_neural_2020,
	location = {Canada -- Quebec, {CA}},
	title = {A Neural Network-Embedded Optimization Approach for Selecting Multiple Entries for March Madness},
	rights = {Database copyright {ProQuest} {LLC}; {ProQuest} does not claim copyright in the individual underlying works.},
	url = {https://www.proquest.com/docview/2626002651/abstract/E8255EE0092F466CPQ/1},
	abstract = {Sports gambling are expected to grow in popularity in the {US} as they have been legalized by many states in the last two years. The availability of sports data and the development of new metrics to evaluate the performance of either athletes or teams have allowed the use of statistical approaches to tackle decision-making problems in sports. While most papers in the literature investigate how to predict the outcome of a game, this thesis addresses the development of an optimal strategy to win a sports betting contest. Specifically, we focus on the {ESPN} Tournament Challenge which is a sport betting contest on the season-ending championship tournaments of American college basketball, also known as the March Madness. The {ESPN} Tournament Challenge asks participants to pick the winner of each of the 63 games in the March Madness. Thus, there is a total of 263 different ways of filling the tournament which makes the challenge a complex task. Every year, millions of people aim to predict accurately the March Madness. This contest often adopts a top-heavy payoff structure which implies that a single participant needs to beat millions of participant to receive a positive payoff. Kaplan et al. (2001) first introduce an exact approach to the problem by selecting a single-entry that maximizes the expected score. We propose a novel strategy that considers a multi-entry approach to the Tournament Challenge. Such a strategy maximizes the expected score of the maximum scoring entry. We face two main challenge, namely, (1) how to evaluate the objective function and (2) how to optimize it. We then present three approaches for the evaluation of the objective function. This includes an exact approach in a Tree-based algorithm and two approximate models, a simulation approach and a neural network approach. Based on these three different models to evaluate the objective function, we develop both a genetic algorithm and a neural network-embedded algorithm. Finally, we compare the expected score and the empirical score by each approach on each tournament played since 2002. Computational experiments show that the proposed models clearly outperform the single-entry exact approach on every instance.
Alternate abstract:
On peut s’attendre à une croissance en popularité des paris sportifs dans le marché américain suite à la légalisation de ceux-ci dans plusieurs états depuis 2018. De plus, l’augmentation de la quantité de données sur le sport et le développement de nouvelles métriques de performance sportive ont permis depuis quelques années d’avoir une approche statistique pour les problèmes de prise de décision dans le sport. Alors que la littérature sur les paris sportifs couvrent majoritairement des modèles probabilistes pour prédire le résultat d’un évènement, cette thèse s’intéresse plutôt au développement d’une stratégie optimale pour remporter un paris sportif, plus particulièrement le Tournament challenge tenu annuellement par {ESPN}. Le Tournament Challenge demande aux participants de choisir le gagnant de chacune des 63 parties du March Madness, soit le championnat de fin de saison de basketball collégial américain. Il existe 263 façons de sélectionner les gagnants du tournois. De plus, plusieurs millions de personnes y participent à chaque année. Généralement, seulement un petit pourcentage des meilleurs scores font un gain monétaire ce qui implique qu’un participant doit obtenir un meilleur score que plusieurs millions de personnes pour remporter un gain. Kaplan et al. (2001) ont été les premiers à introduire une approche exacte qui maximise l’espérance de point produit par une entrée. Notre stratégie est la première à considérer plusieurs entrées dépendantes au Tournament Challenge. Notre stratégie cherche à maximiser l’espérance de points produit par le score maximal des k entrées. Deux problèmes découlent de cette stratégie, soit comment évaluer et comment optimiser la fonction objective. Nous présentons trois approches pour évaluer la fonction objective. Cela inclue une méthode exacte qui est un algorithme basé sur un arbre de décision et deux modèles approximatifs, soit une approche par simulation et une approche par apprentissage machine. À partir de ces différents modèles, nous développons deux heuristiques permettant d’optimiser la fonction objective, soit un algorithme génétique et un réseau de neurones intégré à un modèle en nombre entier. Finalement, nous comparons l’espérance de points produits ainsi que le vrai score obtenu par chacune des méthodes pour chaque tournois depuis 2002. Nos deux modèles surpassent pour chaque instance la solution optimal du modèle exacte avec une entrée.},
	pagetotal = {67},
	institution = {Ecole Polytechnique, Montreal (Canada)},
	type = {M.Appl.Sc.},
	author = {Sylvestre-Décary, Jeff},
	urldate = {2023-04-28},
	date = {2020},
	note = {{ISBN}: 9798759977698},
	keywords = {Integer programming, Computer science, Machine learning, Applied mathematics, Neural networks, Basketball, Expected values, Legalized gambling, Probability, Statistics, Teams, Tournaments \& championships},
	file = {Full Text PDF:/home/brunompacheco/Zotero/storage/2I4T3ASS/Sylvestre-Décary - 2020 - A Neural Network-Embedded Optimization Approach fo.pdf:application/pdf},
}

@misc{bergmeir_comparison_2022,
	title = {Comparison and Evaluation of Methods for a Predict+Optimize Problem in Renewable Energy},
	url = {http://arxiv.org/abs/2212.10723},
	doi = {10.48550/arXiv.2212.10723},
	abstract = {Algorithms that involve both forecasting and optimization are at the core of solutions to many difficult real-world problems, such as in supply chains (inventory optimization), traffic, and in the transition towards carbon-free energy generation in battery/load/production scheduling in sustainable energy systems. Typically, in these scenarios we want to solve an optimization problem that depends on unknown future values, which therefore need to be forecast. As both forecasting and optimization are difficult problems in their own right, relatively few research has been done in this area. This paper presents the findings of the ``{IEEE}-{CIS} Technical Challenge on Predict+Optimize for Renewable Energy Scheduling," held in 2021. We present a comparison and evaluation of the seven highest-ranked solutions in the competition, to provide researchers with a benchmark problem and to establish the state of the art for this benchmark, with the aim to foster and facilitate research in this area. The competition used data from the Monash Microgrid, as well as weather data and energy market data. It then focused on two main challenges: forecasting renewable energy production and demand, and obtaining an optimal schedule for the activities (lectures) and on-site batteries that lead to the lowest cost of energy. The most accurate forecasts were obtained by gradient-boosted tree and random forest models, and optimization was mostly performed using mixed integer linear and quadratic programming. The winning method predicted different scenarios and optimized over all scenarios jointly using a sample average approximation method.},
	number = {{arXiv}:2212.10723},
	publisher = {{arXiv}},
	author = {Bergmeir, Christoph and de Nijs, Frits and Sriramulu, Abishek and Abolghasemi, Mahdi and Bean, Richard and Betts, John and Bui, Quang and Dinh, Nam Trong and Einecke, Nils and Esmaeilbeigi, Rasul and Ferraro, Scott and Galketiya, Priya and Genov, Evgenii and Glasgow, Robert and Godahewa, Rakshitha and Kang, Yanfei and Limmer, Steffen and Magdalena, Luis and Montero-Manso, Pablo and Peralta, Daniel and Kumar, Yogesh Pipada Sunil and Rosales-Pérez, Alejandro and Ruddick, Julian and Stratigakos, Akylas and Stuckey, Peter and Tack, Guido and Triguero, Isaac and Yuan, Rui},
	urldate = {2023-04-28},
	date = {2022-12-20},
	eprinttype = {arxiv},
	eprint = {2212.10723 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/N526WIX9/Bergmeir et al. - 2022 - Comparison and Evaluation of Methods for a Predict.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/WUBY7DCH/2212.html:text/html},
}

@misc{jungel_learning-based_2023,
	title = {Learning-based Online Optimization for Autonomous Mobility-on-Demand Fleet Control},
	url = {http://arxiv.org/abs/2302.03963},
	doi = {10.48550/arXiv.2302.03963},
	abstract = {Autonomous mobility-on-demand systems are a viable alternative to mitigate many transportation-related externalities in cities, such as rising vehicle volumes in urban areas and transportation-related pollution. However, the success of these systems heavily depends on efficient and effective fleet control strategies. In this context, we study online control algorithms for autonomous mobility-on-demand systems and develop a novel hybrid combinatorial optimization enriched machine learning pipeline which learns online dispatching and rebalancing policies from optimal full-information solutions. We test our hybrid pipeline on large-scale real-world scenarios with different vehicle fleet sizes and various request densities. We show that our approach outperforms state-of-the-art greedy, and model-predictive control approaches with respect to various {KPIs}, e.g., by up to 17.1\% and on average by 6.3\% in terms of realized profit.},
	number = {{arXiv}:2302.03963},
	publisher = {{arXiv}},
	author = {Jungel, Kai and Parmentier, Axel and Schiffer, Maximilian and Vidal, Thibaut},
	urldate = {2023-04-28},
	date = {2023-02-08},
	eprinttype = {arxiv},
	eprint = {2302.03963 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, optimization layers},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/U3F62A53/Jungel et al. - 2023 - Learning-based Online Optimization for Autonomous .pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/RNTXB2LR/2302.html:text/html},
}

@inproceedings{queiroga_10000_2021,
	title = {10,000 optimal {CVRP} solutions for testing machine learning based heuristics},
	url = {https://openreview.net/forum?id=yHiMXKN6nTl},
	abstract = {We introduce a benchmark of 10,000 instances with heterogeneous characteristics for the capacitated vehicle routing problem. We also provide optimal solutions for almost all of them along with a generator to produce additional training and validation data. This benchmark aims to permit a more systematic comparison of machine learning based search algorithms on this important problem. We also emit recommendations regarding the correct use of this dataset.},
	eventtitle = {{AAAI}-22 Workshop on Machine Learning for Operations Research ({ML}4OR)},
	author = {Queiroga, Eduardo and Sadykov, Ruslan and Uchoa, Eduardo and Vidal, Thibaut},
	urldate = {2023-04-28},
	date = {2021-11-24},
	langid = {english},
	keywords = {imitation learning, background, end-to-end},
	file = {Full Text PDF:/home/brunompacheco/Zotero/storage/TJ3664WR/Queiroga et al. - 2021 - 10,000 optimal CVRP solutions for testing machine .pdf:application/pdf},
}

@online{noauthor_euro_nodate,
	title = {{EURO} Meets {NeurIPS} 2022 Vehicle Routing Competition},
	url = {https://euro-neurips-vrp-2022.challenges.ortec.com/},
	urldate = {2023-04-28},
	file = {EURO Meets NeurIPS 2022 Vehicle Routing Competition:/home/brunompacheco/Zotero/storage/FKG8RXDT/euro-neurips-vrp-2022.challenges.ortec.com.html:text/html},
}

@thesis{gomes_santana_exploring_2022,
	location = {Rio de Janeiro, Brazil},
	title = {{EXPLORING} {THE} {FRONTIER} {OF} {COMBINATORIAL} {OPTIMIZATION} {AND} {MACHINE} {LEARNING}: {APPLICATIONS} {TO} {VEHICLE} {ROUTING} {AND} {SUPPORT} {VECTOR} {MACHINES}},
	url = {http://www.maxwell.vrac.puc-rio.br/Busca_etds.php?strSecao=resultado&nrSeq=61096@2},
	shorttitle = {{EXPLORING} {THE} {FRONTIER} {OF} {COMBINATORIAL} {OPTIMIZATION} {AND} {MACHINE} {LEARNING}},
	abstract = {Santana, Ítalo Gomes; Vidal, Thibaut Victor Gaston (Advisor). Exploring the frontier of Combinatorial Optimization and Machine Learning: Applications to Vehicle Routing and Support Vector Machines. Rio de Janeiro, 2022. 93p. Tese de Doutorado – Departamento de Informática, Pontifícia Universidade Católica do Rio de Janeiro.},
	institution = {{PONTIFÍCIA} {UNIVERSIDADE} {CATÓLICA} {DO} {RIO} {DE} {JANEIRO}},
	type = {{DOUTOR} {EM} {CIÊNCIAS} - {INFORMÁTICA}},
	author = {Gomes Santana, Italo},
	urldate = {2023-04-28},
	date = {2022-09-09},
	langid = {english},
	doi = {10.17771/PUCRio.acad.61096},
	file = {Gomes Santana - 2022 - EXPLORING THE FRONTIER OF COMBINATORIAL OPTIMIZATI.pdf:/home/brunompacheco/Zotero/storage/WLAY9Y5I/Gomes Santana - 2022 - EXPLORING THE FRONTIER OF COMBINATORIAL OPTIMIZATI.pdf:application/pdf},
}

@misc{quirynen_tailored_2022,
	title = {Tailored Presolve Techniques in Branch-and-Bound Method for Fast Mixed-Integer Optimal Control Applications},
	url = {http://arxiv.org/abs/2211.12700},
	doi = {10.48550/arXiv.2211.12700},
	abstract = {Mixed-integer model predictive control ({MI}-{MPC}) can be a powerful tool for modeling hybrid control systems. In case of a linear-quadratic objective in combination with linear or piecewise-linear system dynamics and inequality constraints, {MI}-{MPC} needs to solve a mixed-integer quadratic program ({MIQP}) at each sampling time step. This paper presents a collection of block-sparse presolve techniques to efficiently remove decision variables, and to remove or tighten inequality constraints, tailored to mixed-integer optimal control problems ({MIOCP}). In addition, we describe a novel heuristic approach based on an iterative presolve algorithm to compute a feasible but possibly suboptimal {MIQP} solution. We present benchmarking results for a C code implementation of the proposed {BB}-{ASIPM} solver, including a branch-and-bound (B\&B) method with the proposed tailored presolve techniques and an active-set based interior point method ({ASIPM}), compared against multiple state-of-the-art {MIQP} solvers on a case study of motion planning with obstacle avoidance constraints. Finally, we demonstrate the computational performance of the {BB}-{ASIPM} solver on the {dSPACE} Scalexio real-time embedded hardware using a second case study of stabilization for an underactuated cart-pole with soft contacts.},
	number = {{arXiv}:2211.12700},
	publisher = {{arXiv}},
	author = {Quirynen, Rien and Di Cairano, Stefano},
	urldate = {2023-04-28},
	date = {2022-11-22},
	eprinttype = {arxiv},
	eprint = {2211.12700 [cs, eess, math]},
	keywords = {Mathematics - Optimization and Control, Electrical Engineering and Systems Science - Systems and Control, Pred-and-opt},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/GAG97C3F/Quirynen and Di Cairano - 2022 - Tailored Presolve Techniques in Branch-and-Bound M.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/QXCZL8D8/2211.html:text/html},
}

@misc{maragno_mixed-integer_2023,
	title = {Mixed-Integer Optimization with Constraint Learning},
	url = {http://arxiv.org/abs/2111.04469},
	doi = {10.48550/arXiv.2111.04469},
	abstract = {We establish a broad methodological foundation for mixed-integer optimization with learned constraints. We propose an end-to-end pipeline for data-driven decision making in which constraints and objectives are directly learned from data using machine learning, and the trained models are embedded in an optimization formulation. We exploit the mixed-integer optimization-representability of many machine learning methods, including linear models, decision trees, ensembles, and multi-layer perceptrons, which allows us to capture various underlying relationships between decisions, contextual variables, and outcomes. We also introduce two approaches for handling the inherent uncertainty of learning from data. First, we characterize a decision trust region using the convex hull of the observations, to ensure credible recommendations and avoid extrapolation. We efficiently incorporate this representation using column generation and propose a more flexible formulation to deal with low-density regions and high-dimensional datasets. Then, we propose an ensemble learning approach that enforces constraint satisfaction over multiple bootstrapped estimators or multiple algorithms. In combination with domain-driven components, the embedded models and trust region define a mixed-integer optimization problem for prescription generation. We implement this framework as a Python package ({OptiCL}) for practitioners. We demonstrate the method in both World Food Programme planning and chemotherapy optimization. The case studies illustrate the framework's ability to generate high-quality prescriptions as well as the value added by the trust region, the use of ensembles to control model robustness, the consideration of multiple machine learning methods, and the inclusion of multiple learned constraints.},
	number = {{arXiv}:2111.04469},
	publisher = {{arXiv}},
	author = {Maragno, Donato and Wiberg, Holly and Bertsimas, Dimitris and Birbil, S. Ilker and Hertog, Dick den and Fajemisin, Adejuyigbe},
	urldate = {2023-04-28},
	date = {2023-01-19},
	eprinttype = {arxiv},
	eprint = {2111.04469 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/6D8BYEZD/Maragno et al. - 2023 - Mixed-Integer Optimization with Constraint Learnin.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/EBCBY26I/2111.html:text/html},
}

@inproceedings{gasse_machine_2022,
	title = {The Machine Learning for Combinatorial Optimization Competition ({ML}4CO): Results and Insights},
	url = {https://proceedings.mlr.press/v176/gasse22a.html},
	shorttitle = {The Machine Learning for Combinatorial Optimization Competition ({ML}4CO)},
	abstract = {Combinatorial optimization is a well-established area in operations research and computer science. Until recently, its methods have focused on solving problem instances in isolation, ignoring that they often stem from related data distributions in practice. However, recent years have seen a surge of interest in using machine learning as a new approach for solving combinatorial problems, either directly as solvers or by enhancing exact solvers. Based on this context, the {ML}4CO aims at improving state-of-the-art combinatorial optimization solvers by replacing key heuristic components. The competition featured three challenging tasks: finding the best feasible solution, producing the tightest optimality certificate, and giving an appropriate solver configuration. Three realistic datasets were considered: balanced item placement, workload apportionment, and maritime inventory routing. This last dataset was kept anonymous for the contestants.},
	eventtitle = {{NeurIPS} 2021 Competitions and Demonstrations Track},
	pages = {220--231},
	booktitle = {Proceedings of the {NeurIPS} 2021 Competitions and Demonstrations Track},
	publisher = {{PMLR}},
	author = {Gasse, Maxime and Bowly, Simon and Cappart, Quentin and Charfreitag, Jonas and Charlin, Laurent and Chételat, Didier and Chmiela, Antonia and Dumouchelle, Justin and Gleixner, Ambros and Kazachkov, Aleksandr M. and Khalil, Elias and Lichocki, Pawel and Lodi, Andrea and Lubin, Miles and Maddison, Chris J. and Christopher, Morris and Papageorgiou, Dimitri J. and Parjadis, Augustin and Pokutta, Sebastian and Prouvost, Antoine and Scavuzzo, Lara and Zarpellon, Giulia and Yang, Linxin and Lai, Sha and Wang, Akang and Luo, Xiaodong and Zhou, Xiang and Huang, Haohan and Shao, Shengcheng and Zhu, Yuanming and Zhang, Dong and Quan, Tao and Cao, Zixuan and Xu, Yang and Huang, Zhewei and Zhou, Shuchang and Binbin, Chen and Minggui, He and Hao, Hao and Zhiyu, Zhang and Zhiwu, An and Kun, Mao},
	urldate = {2023-04-28},
	date = {2022-07-20},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	keywords = {*, imitation learning, end-to-end},
	file = {Full Text PDF:/home/brunompacheco/Zotero/storage/CSBKLQR4/Gasse et al. - 2022 - The Machine Learning for Combinatorial Optimizatio.pdf:application/pdf},
}

@inproceedings{song_general_2020,
	title = {A General Large Neighborhood Search Framework for Solving Integer Linear Programs},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/e769e03a9d329b2e864b4bf4ff54ff39-Abstract.html},
	abstract = {This paper studies how to design abstractions of large-scale combinatorial optimization problems that can leverage existing state-of-the-art solvers in general-purpose ways, and that are amenable to data-driven design.  The goal is to arrive at new approaches that can reliably outperform existing solvers in wall-clock time.  We focus on solving integer programs and ground our approach in the large neighborhood search ({LNS}) paradigm, which iteratively chooses a subset of variables to optimize while leaving the remainder fixed.  The appeal of {LNS} is that it can easily use any existing solver as a subroutine, and thus can inherit the benefits of carefully engineered heuristic approaches and their software implementations.  We also show that one can learn a good neighborhood selector from training data.  Through an extensive empirical validation, we demonstrate that our {LNS} framework can significantly outperform, in wall-clock time, compared to state-of-the-art commercial solvers such as Gurobi.},
	pages = {20012--20023},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Song, Jialin and lanka, ravi and Yue, Yisong and Dilkina, Bistra},
	urldate = {2023-05-08},
	date = {2020},
	keywords = {Pred-and-opt, neighborhood search},
	file = {Full Text PDF:/home/brunompacheco/Zotero/storage/MTH3GDPR/Song et al. - 2020 - A General Large Neighborhood Search Framework for .pdf:application/pdf},
}

@misc{nair_solving_2021,
	title = {Solving Mixed Integer Programs Using Neural Networks},
	url = {http://arxiv.org/abs/2012.13349},
	doi = {10.48550/arXiv.2012.13349},
	abstract = {Mixed Integer Programming ({MIP}) solvers rely on an array of sophisticated heuristics developed with decades of research to solve large-scale {MIP} instances encountered in practice. Machine learning offers to automatically construct better heuristics from data by exploiting shared structure among instances in the data. This paper applies learning to the two key sub-tasks of a {MIP} solver, generating a high-quality joint variable assignment, and bounding the gap in objective value between that assignment and an optimal one. Our approach constructs two corresponding neural network-based components, Neural Diving and Neural Branching, to use in a base {MIP} solver such as {SCIP}. Neural Diving learns a deep neural network to generate multiple partial assignments for its integer variables, and the resulting smaller {MIPs} for un-assigned variables are solved with {SCIP} to construct high quality joint assignments. Neural Branching learns a deep neural network to make variable selection decisions in branch-and-bound to bound the objective value gap with a small tree. This is done by imitating a new variant of Full Strong Branching we propose that scales to large instances using {GPUs}. We evaluate our approach on six diverse real-world datasets, including two Google production datasets and {MIPLIB}, by training separate neural networks on each. Most instances in all the datasets combined have \$10{\textasciicircum}3-10{\textasciicircum}6\$ variables and constraints after presolve, which is significantly larger than previous learning approaches. Comparing solvers with respect to primal-dual gap averaged over a held-out set of instances, the learning-augmented {SCIP} is 2x to 10x better on all datasets except one on which it is \$10{\textasciicircum}5\$x better, at large time limits. To the best of our knowledge, ours is the first learning approach to demonstrate such large improvements over {SCIP} on both large-scale real-world application datasets and {MIPLIB}.},
	number = {{arXiv}:2012.13349},
	publisher = {{arXiv}},
	author = {Nair, Vinod and Bartunov, Sergey and Gimeno, Felix and von Glehn, Ingrid and Lichocki, Pawel and Lobov, Ivan and O'Donoghue, Brendan and Sonnerat, Nicolas and Tjandraatmadja, Christian and Wang, Pengming and Addanki, Ravichandra and Hapuarachchi, Tharindi and Keck, Thomas and Keeling, James and Kohli, Pushmeet and Ktena, Ira and Li, Yujia and Vinyals, Oriol and Zwols, Yori},
	urldate = {2023-05-08},
	date = {2021-07-29},
	eprinttype = {arxiv},
	eprint = {2012.13349 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Discrete Mathematics, *, imitation learning, end-to-end},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/MD3RE8ZB/Nair et al. - 2021 - Solving Mixed Integer Programs Using Neural Networ.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/XCLV62TP/2012.html:text/html},
}

@article{khalil_learning_2017,
	title = {Learning to Run Heuristics in Tree Search},
	url = {https://www.ijcai.org/proceedings/2017/92},
	abstract = {Electronic proceedings of {IJCAI} 2017},
	pages = {659--666},
	author = {Khalil, Elias B. and Dilkina, Bistra and Nemhauser, George L. and Ahmed, Shabbir and Shao, Yufen},
	urldate = {2023-05-08},
	date = {2017},
	keywords = {*, primal heuristics},
	file = {Khalil et al. - 2017 - Learning to Run Heuristics in Tree Search.pdf:/home/brunompacheco/Zotero/storage/Q4KAYVVA/Khalil et al. - 2017 - Learning to Run Heuristics in Tree Search.pdf:application/pdf;Snapshot:/home/brunompacheco/Zotero/storage/57YDXHB4/92.html:text/html},
}

@inproceedings{kotary_end--end_2021,
	location = {Montreal, Canada},
	title = {End-to-End Constrained Optimization Learning: A Survey},
	isbn = {978-0-9992411-9-6},
	url = {https://www.ijcai.org/proceedings/2021/610},
	doi = {10.24963/ijcai.2021/610},
	shorttitle = {End-to-End Constrained Optimization Learning},
	abstract = {This paper surveys the recent attempts at leveraging machine learning to solve constrained optimization problems. It focuses on surveying the work on integrating combinatorial solvers and optimization methods with machine learning architectures.

These approaches hold the promise to develop new hybrid machine learning and optimization methods to predict fast, approximate, solutions to combinatorial problems and to enable structural logical inference. This paper presents a conceptual review of the recent advancements in this emerging area.},
	eventtitle = {Thirtieth International Joint Conference on Artificial Intelligence \{{IJCAI}-21\}},
	pages = {4475--4482},
	booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Kotary, James and Fioretto, Ferdinando and Van Hentenryck, Pascal and Wilder, Bryan},
	urldate = {2023-05-10},
	date = {2021-08},
	langid = {english},
	keywords = {{SKIMMED}, *, optimization layers, constraint learning, background},
	file = {Full Text:/home/brunompacheco/Zotero/storage/MPXYEP4N/Kotary et al. - 2021 - End-to-End Constrained Optimization Learning A Su.pdf:application/pdf},
}

@article{prat_learning_2023,
	title = {Learning Active Constraints to Efficiently Solve Linear Bilevel Problems: Application to the Generator Strategic Bidding Problem},
	volume = {38},
	issn = {1558-0679},
	doi = {10.1109/TPWRS.2022.3188432},
	shorttitle = {Learning Active Constraints to Efficiently Solve Linear Bilevel Problems},
	abstract = {Bilevel programming can be used to formulate many problems in the field of power systems, such as strategic bidding. However, common reformulations of bilevel problems to mixed-integer linear programs make solving such problems hard, which impedes their implementation in real-life. In this paper, we significantly improve solution speed and tractability by introducing decision trees to learn the active constraints of the lower-level problem, while avoiding to introduce binaries and big-M constants. The application of machine learning reduces the online solving time, by moving the selection of active constraints to an offline process, and becomes particularly beneficial when the same problem has to be solved multiple times. We apply our approach to the strategic bidding of generators in electricity markets, where generators solve the same problem many times for varying load demand or renewable production. Three methods are developed and applied to the problem of a strategic generator, with a {DCOPF} in the lower-level. These methods are heuristic and as so, do not provide guarantees of optimality or solution quality. Yet, we show that for networks of varying sizes, the computational burden is significantly reduced, while we also manage to find solutions for strategic bidding problems that were previously intractable.},
	pages = {2376--2387},
	number = {3},
	journaltitle = {{IEEE} Transactions on Power Systems},
	author = {Prat, Eléa and Chatzivasileiadis, Spyros},
	date = {2023-05},
	note = {Conference Name: {IEEE} Transactions on Power Systems},
	keywords = {Machine learning, Generators, Voltage, active set, Bilevel programming, classifier, Costs, Games, mixed-integer linear programming ({MILP}), Power systems, Runtime, stackelberg games, strategic bidding},
	file = {IEEE Xplore Abstract Record:/home/brunompacheco/Zotero/storage/B47SRJUK/9815140.html:text/html;IEEE Xplore Full Text PDF:/home/brunompacheco/Zotero/storage/UGG5M9JJ/Prat and Chatzivasileiadis - 2023 - Learning Active Constraints to Efficiently Solve L.pdf:application/pdf},
}

@article{detassis_teaching_2021,
	title = {Teaching the Old Dog New Tricks: Supervised Learning with Constraints},
	volume = {35},
	rights = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16491},
	doi = {10.1609/aaai.v35i5.16491},
	shorttitle = {Teaching the Old Dog New Tricks},
	abstract = {Adding constraint support in Machine Learning has the potential to address outstanding issues in data-driven {AI} systems, such as safety and fairness. Existing approaches typically apply constrained optimization techniques to {ML} training, enforce constraint satisfaction by adjusting the model design, or use constraints to correct the output. Here, we investigate a different, complementary, strategy based on "teaching" constraint satisfaction to a supervised {ML} method via the direct use of a state-of-the-art constraint solver: this enables taking advantage of decades of research on constrained optimization with limited effort. In practice, we use a decomposition scheme alternating master steps (in charge of enforcing the constraints) and learner steps (where any supervised {ML} model and training algorithm can be employed). The process leads to approximate constraint satisfaction in general, and convergence properties are difficult to establish; despite this fact, we found empirically that even a naive setup of our approach performs well on {ML} tasks with fairness constraints, and on classical datasets with synthetic constraints.},
	pages = {3742--3749},
	number = {5},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Detassis, Fabrizio and Lombardi, Michele and Milano, Michela},
	urldate = {2023-05-10},
	date = {2021-05-18},
	langid = {english},
	note = {Number: 5},
	keywords = {imitation learning, constraint learning, background},
	file = {Full Text PDF:/home/brunompacheco/Zotero/storage/SACZPEUX/Detassis et al. - 2021 - Teaching the Old Dog New Tricks Supervised Learni.pdf:application/pdf},
}

@inproceedings{vinyals_pointer_2015,
	title = {Pointer Networks},
	volume = {28},
	url = {https://papers.nips.cc/paper_files/paper/2015/hash/29921001f2f04bd3baee84a12e98098f-Abstract.html},
	abstract = {We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that arediscrete tokens corresponding to positions in an input sequence.Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines,because the number of target classes in eachstep of the output depends on the length of the input, which is variable.Problems such as sorting variable sized sequences, and various combinatorialoptimization problems belong to this class.  Our model solvesthe problem of variable size output dictionaries using a recently proposedmechanism of neural attention. It differs from the previous attentionattempts in that, instead of using attention to blend hidden units of anencoder to a context vector at each decoder step, it uses attention asa pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net).We show Ptr-Nets can be used to learn approximate solutions to threechallenging geometric problems -- finding planar convex hulls, {computingDelaunay} triangulations, and the planar Travelling Salesman Problem-- using training examples alone. Ptr-Nets not only improve oversequence-to-sequence with input attention, butalso allow us to generalize to variable size output dictionaries.We show that the learnt models generalize beyond the maximum lengthsthey were trained on. We hope our results on these taskswill encourage a broader exploration of neural learning for discreteproblems.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
	urldate = {2023-05-10},
	date = {2015},
	keywords = {*, imitation learning, background, end-to-end},
	file = {Full Text PDF:/home/brunompacheco/Zotero/storage/B52L73AJ/Vinyals et al. - 2015 - Pointer Networks.pdf:application/pdf},
}

@misc{wilder_melding_2018,
	title = {Melding the Data-Decisions Pipeline: Decision-Focused Learning for Combinatorial Optimization},
	url = {http://arxiv.org/abs/1809.05504},
	doi = {10.48550/arXiv.1809.05504},
	shorttitle = {Melding the Data-Decisions Pipeline},
	abstract = {Creating impact in real-world settings requires artificial intelligence techniques to span the full pipeline from data, to predictive models, to decisions. These components are typically approached separately: a machine learning model is first trained via a measure of predictive accuracy, and then its predictions are used as input into an optimization algorithm which produces a decision. However, the loss function used to train the model may easily be misaligned with the end goal, which is to make the best decisions possible. Hand-tuning the loss function to align with optimization is a difficult and error-prone process (which is often skipped entirely). We focus on combinatorial optimization problems and introduce a general framework for decision-focused learning, where the machine learning model is directly trained in conjunction with the optimization algorithm to produce high-quality decisions. Technically, our contribution is a means of integrating common classes of discrete optimization problems into deep learning or other predictive models, which are typically trained via gradient descent. The main idea is to use a continuous relaxation of the discrete problem to propagate gradients through the optimization procedure. We instantiate this framework for two broad classes of combinatorial problems: linear programs and submodular maximization. Experimental results across a variety of domains show that decision-focused learning often leads to improved optimization performance compared to traditional methods. We find that standard measures of accuracy are not a reliable proxy for a predictive model's utility in optimization, and our method's ability to specify the true goal as the model's training objective yields substantial dividends across a range of decision problems.},
	number = {{arXiv}:1809.05504},
	publisher = {{arXiv}},
	author = {Wilder, Bryan and Dilkina, Bistra and Tambe, Milind},
	urldate = {2023-05-11},
	date = {2018-11-20},
	eprinttype = {arxiv},
	eprint = {1809.05504 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, optimization layers},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/2RHEQIQY/Wilder et al. - 2018 - Melding the Data-Decisions Pipeline Decision-Focu.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/GILY8SBF/1809.html:text/html},
}

@misc{berthet_learning_2020,
	title = {Learning with Differentiable Perturbed Optimizers},
	url = {http://arxiv.org/abs/2002.08676},
	doi = {10.48550/arXiv.2002.08676},
	abstract = {Machine learning pipelines often rely on optimization procedures to make discrete decisions (e.g., sorting, picking closest neighbors, or shortest paths). Although these discrete decisions are easily computed, they break the back-propagation of computational graphs. In order to expand the scope of learning problems that can be solved in an end-to-end fashion, we propose a systematic method to transform optimizers into operations that are differentiable and never locally constant. Our approach relies on stochastically perturbed optimizers, and can be used readily together with existing solvers. Their derivatives can be evaluated efficiently, and smoothness tuned via the chosen noise amplitude. We also show how this framework can be connected to a family of losses developed in structured prediction, and give theoretical guarantees for their use in learning tasks. We demonstrate experimentally the performance of our approach on various tasks.},
	number = {{arXiv}:2002.08676},
	publisher = {{arXiv}},
	author = {Berthet, Quentin and Blondel, Mathieu and Teboul, Olivier and Cuturi, Marco and Vert, Jean-Philippe and Bach, Francis},
	urldate = {2023-05-11},
	date = {2020-06-09},
	eprinttype = {arxiv},
	eprint = {2002.08676 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning, optimization layers},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/ZUGXGT2M/Berthet et al. - 2020 - Learning with Differentiable Perturbed Optimizers.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/BPULR8UN/2002.html:text/html},
}

@misc{ferber_mipaal_2019,
	title = {{MIPaaL}: Mixed Integer Program as a Layer},
	url = {http://arxiv.org/abs/1907.05912},
	doi = {10.48550/arXiv.1907.05912},
	shorttitle = {{MIPaaL}},
	abstract = {Machine learning components commonly appear in larger decision-making pipelines; however, the model training process typically focuses only on a loss that measures accuracy between predicted values and ground truth values. Decision-focused learning explicitly integrates the downstream decision problem when training the predictive model, in order to optimize the quality of decisions induced by the predictions. It has been successfully applied to several limited combinatorial problem classes, such as those that can be expressed as linear programs ({LP}), and submodular optimization. However, these previous applications have uniformly focused on problems from specific classes with simple constraints. Here, we enable decision-focused learning for the broad class of problems that can be encoded as a Mixed Integer Linear Program ({MIP}), hence supporting arbitrary linear constraints over discrete and continuous variables. We show how to differentiate through a {MIP} by employing a cutting planes solution approach, which is an exact algorithm that iteratively adds constraints to a continuous relaxation of the problem until an integral solution is found. We evaluate our new end-to-end approach on several real world domains and show that it outperforms the standard two phase approaches that treat prediction and prescription separately, as well as a baseline approach of simply applying decision-focused learning to the {LP} relaxation of the {MIP}.},
	number = {{arXiv}:1907.05912},
	publisher = {{arXiv}},
	author = {Ferber, Aaron and Wilder, Bryan and Dilkina, Bistra and Tambe, Milind},
	urldate = {2023-05-11},
	date = {2019-07-17},
	eprinttype = {arxiv},
	eprint = {1907.05912 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, optimization layers},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/SYQMTKAX/Ferber et al. - 2019 - MIPaaL Mixed Integer Program as a Layer.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/8XWPYTRF/1907.html:text/html},
}

@misc{gilmer_neural_2017,
	title = {Neural Message Passing for Quantum Chemistry},
	url = {http://arxiv.org/abs/1704.01212},
	doi = {10.48550/arXiv.1704.01212},
	abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks ({MPNNs}) and explore additional novel variations within this framework. Using {MPNNs} we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
	number = {{arXiv}:1704.01212},
	publisher = {{arXiv}},
	author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
	urldate = {2023-05-17},
	date = {2017-06-12},
	eprinttype = {arxiv},
	eprint = {1704.01212 [cs]},
	keywords = {Computer Science - Machine Learning, I.2.6, background},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/FXY7S9BY/Gilmer et al. - 2017 - Neural Message Passing for Quantum Chemistry.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/ZCZRU5VJ/1704.html:text/html},
}

@misc{cappart_combinatorial_2022,
	title = {Combinatorial optimization and reasoning with graph neural networks},
	url = {http://arxiv.org/abs/2102.09544},
	doi = {10.48550/arXiv.2102.09544},
	abstract = {Combinatorial optimization is a well-established area in operations research and computer science. Until recently, its methods have focused on solving problem instances in isolation, ignoring that they often stem from related data distributions in practice. However, recent years have seen a surge of interest in using machine learning, especially graph neural networks ({GNNs}), as a key building block for combinatorial tasks, either directly as solvers or by enhancing exact solvers. The inductive bias of {GNNs} effectively encodes combinatorial and relational input due to their invariance to permutations and awareness of input sparsity. This paper presents a conceptual review of recent key advancements in this emerging field, aiming at optimization and machine learning researchers.},
	number = {{arXiv}:2102.09544},
	publisher = {{arXiv}},
	author = {Cappart, Quentin and Chételat, Didier and Khalil, Elias and Lodi, Andrea and Morris, Christopher and Veličković, Petar},
	urldate = {2023-05-17},
	date = {2022-09-23},
	eprinttype = {arxiv},
	eprint = {2102.09544 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, {SURVEY}, {GNN}, *, Computer Science - Data Structures and Algorithms},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/9BSTYTWV/Cappart et al. - 2022 - Combinatorial optimization and reasoning with grap.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/W3D6JZUI/2102.html:text/html},
}

@article{prates_learning_2019,
	title = {Learning to Solve {NP}-Complete Problems: A Graph Neural Network for Decision {TSP}},
	volume = {33},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4399},
	doi = {10.1609/aaai.v33i01.33014731},
	shorttitle = {Learning to Solve {NP}-Complete Problems},
	abstract = {Graph Neural Networks ({GNN}) are a promising technique for bridging differential programming and combinatorial domains. {GNNs} employ trainable modules which can be assembled in different configurations that reflect the relational structure of each problem instance. In this paper, we show that {GNNs} can learn to solve, with very little supervision, the decision variant of the Traveling Salesperson Problem ({TSP}), a highly relevant {NP}-Complete problem. Our model is trained to function as an effective message-passing algorithm in which edges (embedded with their weights) communicate with vertices for a number of iterations after which the model is asked to decide whether a route with cost {\textless} C exists. We show that such a network can be trained with sets of dual examples: given the optimal tour cost C∗, we produce one decision instance with target cost x\% smaller and one with target cost x\% larger than C∗. We were able to obtain 80\% accuracy training with −2\%,+2\% deviations, and the same trained model can generalize for more relaxed deviations with increasing performance. We also show that the model is capable of generalizing for larger problem sizes. Finally, we provide a method for predicting the optimal route cost within 2\% deviation from the ground truth. In summary, our work shows that Graph Neural Networks are powerful enough to solve {NP}-Complete problems which combine symbolic and numeric data.},
	pages = {4731--4738},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Prates, Marcelo and Avelar, Pedro H. C. and Lemos, Henrique and Lamb, Luis C. and Vardi, Moshe Y.},
	urldate = {2023-05-17},
	date = {2019-07-17},
	keywords = {{GNN}, *},
	file = {Full Text:/home/brunompacheco/Zotero/storage/5CBXYW3A/Prates et al. - 2019 - Learning to Solve NP-Complete Problems A Graph Ne.pdf:application/pdf},
}

@article{morris_weisfeiler_2019,
	title = {Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks},
	volume = {33},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4384},
	doi = {10.1609/aaai.v33i01.33014602},
	shorttitle = {Weisfeiler and Leman Go Neural},
	abstract = {In recent years, graph neural networks ({GNNs}) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, {GNNs} have only been evaluated empirically—showing promising results. The following work investigates {GNNs} from a theoretical point of view and relates them to the 1-dimensional Weisfeiler-Leman graph isomorphism heuristic (1-{WL}). We show that {GNNs} have the same expressiveness as the 1-{WL} in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of {GNNs}, so-called k-dimensional {GNNs} (k-{GNNs}), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.},
	pages = {4602--4609},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Morris, Christopher and Ritzert, Martin and Fey, Matthias and Hamilton, William L. and Lenssen, Jan Eric and Rattan, Gaurav and Grohe, Martin},
	urldate = {2023-05-17},
	date = {2019-07-17},
	keywords = {{GNN}, background},
	file = {Full Text:/home/brunompacheco/Zotero/storage/EYF8VSQ5/Morris et al. - 2019 - Weisfeiler and Leman Go Neural Higher-Order Graph.pdf:application/pdf},
}

@article{kipf_semi-supervised_2016,
	title = {Semi-Supervised Classification with Graph Convolutional Networks},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1609.02907},
	doi = {10.48550/ARXIV.1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	author = {Kipf, Thomas N. and Welling, Max},
	urldate = {2023-05-17},
	date = {2016},
	note = {Publisher: {arXiv}
Version Number: 4},
	keywords = {{FOS}: Computer and information sciences, Machine Learning (cs.{LG}), {GNN}, Machine Learning (stat.{ML}), background},
	file = {Full Text:/home/brunompacheco/Zotero/storage/7CR394P6/Kipf and Welling - 2016 - Semi-Supervised Classification with Graph Convolut.pdf:application/pdf},
}

@article{li_combinatorial_2018,
	title = {Combinatorial Optimization with Graph Convolutional Networks and Guided Tree Search},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1810.10659},
	doi = {10.48550/ARXIV.1810.10659},
	abstract = {We present a learning-based approach to computing solutions for certain {NP}-hard problems. Our approach combines deep learning techniques with useful algorithmic elements from classic heuristics. The central component is a graph convolutional network that is trained to estimate the likelihood, for each vertex in a graph, of whether this vertex is part of the optimal solution. The network is designed and trained to synthesize a diverse set of solutions, which enables rapid exploration of the solution space via tree search. The presented approach is evaluated on four canonical {NP}-hard problems and five datasets, which include benchmark satisfiability problems and real social network graphs with up to a hundred thousand nodes. Experimental results demonstrate that the presented approach substantially outperforms recent deep learning work, and performs on par with highly optimized state-of-the-art heuristic solvers for some {NP}-hard problems. Experiments indicate that our approach generalizes across datasets, and scales to graphs that are orders of magnitude larger than those used during training.},
	author = {Li, Zhuwen and Chen, Qifeng and Koltun, Vladlen},
	urldate = {2023-05-17},
	date = {2018},
	note = {Publisher: {arXiv}
Version Number: 1},
	keywords = {{FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Artificial Intelligence (cs.{AI}), Machine Learning (stat.{ML})},
	file = {Full Text:/home/brunompacheco/Zotero/storage/ASMXFEB6/Li et al. - 2018 - Combinatorial Optimization with Graph Convolutiona.pdf:application/pdf},
}

@article{bother_whats_2022,
	title = {What's Wrong with Deep Learning in Tree Search for Combinatorial Optimization},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2201.10494},
	doi = {10.48550/ARXIV.2201.10494},
	abstract = {Combinatorial optimization lies at the core of many real-world problems. Especially since the rise of graph neural networks ({GNNs}), the deep learning community has been developing solvers that derive solutions to {NP}-hard problems by learning the problem-specific solution structure. However, reproducing the results of these publications proves to be difficult. We make three contributions. First, we present an open-source benchmark suite for the {NP}-hard Maximum Independent Set problem, in both its weighted and unweighted variants. The suite offers a unified interface to various state-of-the-art traditional and machine learning-based solvers. Second, using our benchmark suite, we conduct an in-depth analysis of the popular guided tree search algorithm by Li et al. [{NeurIPS} 2018], testing various configurations on small and large synthetic and real-world graphs. By re-implementing their algorithm with a focus on code quality and extensibility, we show that the graph convolution network used in the tree search does not learn a meaningful representation of the solution structure, and can in fact be replaced by random values. Instead, the tree search relies on algorithmic techniques like graph kernelization to find good solutions. Thus, the results from the original publication are not reproducible. Third, we extend the analysis to compare the tree search implementations to other solvers, showing that the classical algorithmic solvers often are faster, while providing solutions of similar quality. Additionally, we analyze a recent solver based on reinforcement learning and observe that for this solver, the {GNN} is responsible for the competitive solution quality.},
	author = {Böther, Maximilian and Kißig, Otto and Taraz, Martin and Cohen, Sarel and Seidel, Karen and Friedrich, Tobias},
	urldate = {2023-05-17},
	date = {2022},
	note = {Publisher: {arXiv}
Version Number: 1},
	keywords = {{FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Pred-and-opt, Artificial Intelligence (cs.{AI}), background, {FOS}: Mathematics, Optimization and Control (math.{OC}), end-to-end},
	file = {Full Text:/home/brunompacheco/Zotero/storage/RL2WRRVQ/Böther et al. - 2022 - What's Wrong with Deep Learning in Tree Search for.pdf:application/pdf},
}

@article{ding_accelerating_2020,
	title = {Accelerating Primal Solution Findings for Mixed Integer Programs Based on Solution Prediction},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5503},
	doi = {10.1609/aaai.v34i02.5503},
	abstract = {Mixed Integer Programming ({MIP}) is one of the most widely used modeling techniques for combinatorial optimization problems. In many applications, a similar {MIP} model is solved on a regular basis, maintaining remarkable similarities in model structures and solution appearances but differing in formulation coefficients. This offers the opportunity for machine learning methods to explore the correlations between model structures and the resulting solution values. To address this issue, we propose to represent a {MIP} instance using a tripartite graph, based on which a Graph Convolutional Network ({GCN}) is constructed to predict solution values for binary variables. The predicted solutions are used to generate a local branching type cut which can be either treated as a global (invalid) inequality in the formulation resulting in a heuristic approach to solve the {MIP}, or as a root branching rule resulting in an exact approach. Computational evaluations on 8 distinct types of {MIP} problems show that the proposed framework improves the primal solution finding performance significantly on a state-of-the-art open-source {MIP} solver.},
	pages = {1452--1459},
	number = {2},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Ding, Jian-Ya and Zhang, Chao and Shen, Lei and Li, Shengyin and Wang, Bing and Xu, Yinghui and Song, Le},
	urldate = {2023-05-17},
	date = {2020-04-03},
	keywords = {{GNN}, *, imitation learning, end-to-end},
	file = {Full Text:/home/brunompacheco/Zotero/storage/CJIATS8K/Ding et al. - 2020 - Accelerating Primal Solution Findings for Mixed In.pdf:application/pdf},
}

@misc{loukas_what_2020,
	title = {What graph neural networks cannot learn: depth vs width},
	url = {http://arxiv.org/abs/1907.03199},
	doi = {10.48550/arXiv.1907.03199},
	shorttitle = {What graph neural networks cannot learn},
	abstract = {This paper studies the expressive power of graph neural networks falling within the message-passing framework ({GNNmp}). Two results are presented. First, {GNNmp} are shown to be Turing universal under sufficient conditions on their depth, width, node attributes, and layer expressiveness. Second, it is discovered that {GNNmp} can lose a significant portion of their power when their depth and width is restricted. The proposed impossibility statements stem from a new technique that enables the repurposing of seminal results from distributed computing and leads to lower bounds for an array of decision, optimization, and estimation problems involving graphs. Strikingly, several of these problems are deemed impossible unless the product of a {GNNmp}'s depth and width exceeds a polynomial of the graph size; this dependence remains significant even for tasks that appear simple or when considering approximation.},
	number = {{arXiv}:1907.03199},
	publisher = {{arXiv}},
	author = {Loukas, Andreas},
	urldate = {2023-05-17},
	date = {2020-01-28},
	eprinttype = {arxiv},
	eprint = {1907.03199 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, {GNN}, background},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/B7B3TQXU/Loukas - 2020 - What graph neural networks cannot learn depth vs .pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/7WD9C7IG/1907.html:text/html},
}

@misc{kool_attention_2019,
	title = {Attention, Learn to Solve Routing Problems!},
	url = {http://arxiv.org/abs/1803.08475},
	doi = {10.48550/arXiv.1803.08475},
	abstract = {The recently presented idea to learn heuristics for combinatorial optimization problems is promising as it can save costly development. However, to push this idea towards practical implementation, we need better models and better ways of training. We contribute in both directions: we propose a model based on attention layers with benefits over the Pointer Network and we show how to train this model using {REINFORCE} with a simple baseline based on a deterministic greedy rollout, which we find is more efficient than using a value function. We significantly improve over recent learned heuristics for the Travelling Salesman Problem ({TSP}), getting close to optimal results for problems up to 100 nodes. With the same hyperparameters, we learn strong heuristics for two variants of the Vehicle Routing Problem ({VRP}), the Orienteering Problem ({OP}) and (a stochastic variant of) the Prize Collecting {TSP} ({PCTSP}), outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms.},
	number = {{arXiv}:1803.08475},
	publisher = {{arXiv}},
	author = {Kool, Wouter and van Hoof, Herke and Welling, Max},
	urldate = {2023-05-22},
	date = {2019-02-07},
	eprinttype = {arxiv},
	eprint = {1803.08475 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, end-to-end},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/IH6HA33T/Kool et al. - 2019 - Attention, Learn to Solve Routing Problems!.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/C5M227Z7/1803.html:text/html;Full Text:/home/brunompacheco/Zotero/storage/QFZIQFUD/Kool et al. - 2019 - Attention, Learn to Solve Routing Problems!.pdf:application/pdf},
}

@inproceedings{nowak_revised_2018,
	title = {{REVISED} {NOTE} {ON} {LEARNING} {QUADRATIC} {ASSIGNMENT} {WITH} {GRAPH} {NEURAL} {NETWORKS}},
	doi = {10.1109/DSW.2018.8439919},
	abstract = {Inverse problems correspond to a certain type of optimization problems formulated over appropriate input distributions. Recently, there has been a growing interest in understanding the computational hardness of these optimization problems, not only in the worst case, but in an average-complexity sense under this same input distribution.In this revised note, we are interested in studying another aspect of hardness, related to the ability to learn how to solve a problem by simply observing a collection of previously solved instances. These `planted solutions' are used to supervise the training of an appropriate predictive model that parametrizes a broad class of algorithms, with the hope that the resulting model will provide good accuracy-complexity tradeoffs in the average sense.We illustrate this setup on the Quadratic Assignment Problem, a fundamental problem in Network Science. We observe that data-driven models based on Graph Neural Networks offer intriguingly good performance, even in regimes where standard relaxation based techniques appear to suffer.},
	eventtitle = {2018 {IEEE} Data Science Workshop ({DSW})},
	pages = {1--5},
	booktitle = {2018 {IEEE} Data Science Workshop ({DSW})},
	author = {Nowak, Alex and Villar, Soledad and Bandeira, Afonso S and Bruna, Joan},
	date = {2018-06},
	keywords = {Neural networks, Computer architecture, Complexity theory, Computational modeling, Task analysis, Optimization, {GNN}, imitation learning, end-to-end, Symmetric matrices},
	file = {IEEE Xplore Abstract Record:/home/brunompacheco/Zotero/storage/9UYBRUYN/8439919.html:text/html;Nowak et al. - 2018 - REVISED NOTE ON LEARNING QUADRATIC ASSIGNMENT WITH.pdf:/home/brunompacheco/Zotero/storage/RNVD4FWY/Nowak et al. - 2018 - REVISED NOTE ON LEARNING QUADRATIC ASSIGNMENT WITH.pdf:application/pdf},
}

@article{larsen_predicting_2022,
	title = {Predicting Tactical Solutions to Operational Planning Problems Under Imperfect Information},
	volume = {34},
	issn = {1091-9856},
	url = {https://pubsonline.informs.org/doi/10.1287/ijoc.2021.1091},
	doi = {10.1287/ijoc.2021.1091},
	abstract = {This paper offers a methodological contribution at the intersection of machine learning and operations research. Namely, we propose a methodology to quickly predict expected tactical descriptions of operational solutions ({TDOSs}). The problem we address occurs in the context of two-stage stochastic programming, where the second stage is demanding computationally. We aim to predict at a high speed the expected {TDOS} associated with the second-stage problem, conditionally on the first-stage variables. This may be used in support of the solution to the overall two-stage problem by avoiding the online generation of multiple second-stage scenarios and solutions. We formulate the tactical prediction problem as a stochastic optimal prediction program, whose solution we approximate with supervised machine learning. The training data set consists of a large number of deterministic operational problems generated by controlled probabilistic sampling. The labels are computed based on solutions to these problems (solved independently and offline), employing appropriate aggregation and subselection methods to address uncertainty. Results on our motivating application on load planning for rail transportation show that deep learning models produce accurate predictions in very short computing time (milliseconds or less). The predictive accuracy is close to the lower bounds calculated based on sample average approximation of the stochastic prediction programs.},
	pages = {227--242},
	number = {1},
	journaltitle = {{INFORMS} Journal on Computing},
	author = {Larsen, Eric and Lachapelle, Sébastien and Bengio, Yoshua and Frejinger, Emma and Lacoste-Julien, Simon and Lodi, Andrea},
	urldate = {2023-05-22},
	date = {2022-01},
	note = {Publisher: {INFORMS}},
	keywords = {deep learning, integer linear programming, stochastic programming, supervised learning, imitation learning, end-to-end},
	file = {Submitted Version:/home/brunompacheco/Zotero/storage/2B5MSZDK/Larsen et al. - 2022 - Predicting Tactical Solutions to Operational Plann.pdf:application/pdf},
}

@misc{karalias_erdos_2021,
	title = {Erdos Goes Neural: an Unsupervised Learning Framework for Combinatorial Optimization on Graphs},
	url = {http://arxiv.org/abs/2006.10643},
	doi = {10.48550/arXiv.2006.10643},
	shorttitle = {Erdos Goes Neural},
	abstract = {Combinatorial optimization problems are notoriously challenging for neural networks, especially in the absence of labeled instances. This work proposes an unsupervised learning framework for {CO} problems on graphs that can provide integral solutions of certified quality. Inspired by Erdos' probabilistic method, we use a neural network to parametrize a probability distribution over sets. Crucially, we show that when the network is optimized w.r.t. a suitably chosen loss, the learned distribution contains, with controlled probability, a low-cost integral solution that obeys the constraints of the combinatorial problem. The probabilistic proof of existence is then derandomized to decode the desired solutions. We demonstrate the efficacy of this approach to obtain valid solutions to the maximum clique problem and to perform local graph clustering. Our method achieves competitive results on both real datasets and synthetic hard instances.},
	number = {{arXiv}:2006.10643},
	publisher = {{arXiv}},
	author = {Karalias, Nikolaos and Loukas, Andreas},
	urldate = {2023-05-23},
	date = {2021-03-07},
	eprinttype = {arxiv},
	eprint = {2006.10643 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/SWBXD2VI/Karalias and Loukas - 2021 - Erdos Goes Neural an Unsupervised Learning Framew.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/SQJK92ER/2006.html:text/html},
}

@article{hottung_neural_2022,
	title = {Neural large neighborhood search for routing problems},
	volume = {313},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370222001266},
	doi = {10.1016/j.artint.2022.103786},
	abstract = {Learning how to automatically solve optimization problems has the potential to provide the next big leap in optimization technology. The performance of automatically learned heuristics on routing problems has been steadily improving in recent years, but approaches based purely on machine learning are still outperformed by state-of-the-art optimization methods. To close this performance gap, we propose a novel large neighborhood search ({LNS}) framework for vehicle routing that integrates learned heuristics for generating new solutions. The learning mechanism is based on a deep neural network with an attention mechanism and has been especially designed to be integrated into an {LNS} search setting. We evaluate our approach on the capacitated vehicle routing problem ({CVRP}), the split delivery vehicle routing problem ({SDVRP}), and the capacitated team orienteering problem ({CTOP}). We show that the {NLNS} approach is able to outperform a handcrafted {LNS} on the {CVRP} and {SDVRP} and match the performance of a standard {LNS} on the {CTOP}. {NLNS} is thus able to quickly and effectively learn high performance heuristics to maneuver through the search space of difficult routing problems, coming close to the performance of state-of-the-art optimization approaches.},
	pages = {103786},
	journaltitle = {Artificial Intelligence},
	shortjournal = {Artificial Intelligence},
	author = {Hottung, André and Tierney, Kevin},
	urldate = {2023-05-23},
	date = {2022-12-01},
	langid = {english},
	keywords = {Reinforcement learning, Combinatorial optimization, neighborhood search, Heuristic search, Learning to optimize, Routing problems},
	file = {ScienceDirect Full Text PDF:/home/brunompacheco/Zotero/storage/L2PZ9RN4/Hottung and Tierney - 2022 - Neural large neighborhood search for routing probl.pdf:application/pdf;ScienceDirect Snapshot:/home/brunompacheco/Zotero/storage/B42Z3X3B/S0004370222001266.html:text/html},
}

@inproceedings{wu_learning_2021,
	title = {Learning Large Neighborhood Search Policy for Integer Programming},
	url = {https://openreview.net/forum?id=IaM7U4J-w3c},
	abstract = {We propose a deep reinforcement learning ({RL}) method to learn large neighborhood search ({LNS}) policy for integer programming ({IP}). The {RL} policy is trained as the destroy operator to select a subset of variables at each step, which is reoptimized by an {IP} solver as the repair operator. However, the combinatorial number of variable subsets prevents direct application of typical {RL} algorithms. To tackle this challenge, we represent all subsets by factorizing them into binary decisions on each variable. We then design a neural network to learn policies for each variable in parallel, trained by a customized actor-critic algorithm. We evaluate the proposed method on four representative {IP} problems. Results show that it can find better solutions than {SCIP} in much less time, and significantly outperform other {LNS} baselines with the same runtime. Moreover, these advantages notably persist when the policies generalize to larger problems. Further experiments with Gurobi also reveal that our method can outperform this state-of-the-art commercial solver within the same time limit.},
	eventtitle = {Advances in Neural Information Processing Systems},
	author = {Wu, Yaoxin and Song, Wen and Cao, Zhiguang and Zhang, Jie},
	urldate = {2023-05-23},
	date = {2021-11-09},
	langid = {english},
	keywords = {neighborhood search},
	file = {Full Text PDF:/home/brunompacheco/Zotero/storage/XK94N2K7/Wu et al. - 2021 - Learning Large Neighborhood Search Policy for Inte.pdf:application/pdf},
}

@misc{sonnerat_learning_2022,
	title = {Learning a Large Neighborhood Search Algorithm for Mixed Integer Programs},
	url = {http://arxiv.org/abs/2107.10201},
	doi = {10.48550/arXiv.2107.10201},
	abstract = {Large Neighborhood Search ({LNS}) is a combinatorial optimization heuristic that starts with an assignment of values for the variables to be optimized, and iteratively improves it by searching a large neighborhood around the current assignment. In this paper we consider a learning-based {LNS} approach for mixed integer programs ({MIPs}). We train a Neural Diving model to represent a probability distribution over assignments, which, together with an off-the-shelf {MIP} solver, generates an initial assignment. Formulating the subsequent search steps as a Markov Decision Process, we train a Neural Neighborhood Selection policy to select a search neighborhood at each step, which is searched using a {MIP} solver to find the next assignment. The policy network is trained using imitation learning. We propose a target policy for imitation that, given enough compute resources, is guaranteed to select the neighborhood containing the optimal next assignment amongst all possible choices for the neighborhood of a specified size. Our approach matches or outperforms all the baselines on five real-world {MIP} datasets with large-scale instances from diverse applications, including two production applications at Google. It achieves \$2{\textbackslash}times\$ to \$37.8{\textbackslash}times\$ better average primal gap than the best baseline on three of the datasets at large running times.},
	number = {{arXiv}:2107.10201},
	publisher = {{arXiv}},
	author = {Sonnerat, Nicolas and Wang, Pengming and Ktena, Ira and Bartunov, Sergey and Nair, Vinod},
	urldate = {2023-05-23},
	date = {2022-05-20},
	eprinttype = {arxiv},
	eprint = {2107.10201 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/S7E6YJJQ/Sonnerat et al. - 2022 - Learning a Large Neighborhood Search Algorithm for.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/WTX3HFXP/2107.html:text/html},
}

@misc{li_learning_2022,
	title = {Learning to Accelerate Approximate Methods for Solving Integer Programming via Early Fixing},
	url = {http://arxiv.org/abs/2207.02087},
	doi = {10.48550/arXiv.2207.02087},
	abstract = {Integer programming ({IP}) is an important and challenging problem. Approximate methods have shown promising performance on both effectiveness and efficiency for solving the {IP} problem. However, we observed that a large fraction of variables solved by some iterative approximate methods fluctuate around their final converged discrete states in very long iterations. Inspired by this observation, we aim to accelerate these approximate methods by early fixing these fluctuated variables to their converged states while not significantly harming the solution accuracy. To this end, we propose an early fixing framework along with the approximate method. We formulate the whole early fixing process as a Markov decision process, and train it using imitation learning. A policy network will evaluate the posterior probability of each free variable concerning its discrete candidate states in each block of iterations. Specifically, we adopt the powerful multi-headed attention mechanism in the policy network. Extensive experiments on our proposed early fixing framework are conducted to three different {IP} applications: constrained linear programming, {MRF} energy minimization and sparse adversarial attack. The former one is linear {IP} problem, while the latter two are quadratic {IP} problems. We extend the problem scale from regular size to significantly large size. The extensive experiments reveal the competitiveness of our early fixing framework: the runtime speeds up significantly, while the solution quality does not degrade much, even in some cases it is available to obtain better solutions. Our proposed early fixing framework can be regarded as an acceleration extension of {ADMM} methods for solving integer programming. The source codes are available at {\textbackslash}url\{https://github.com/{SCLBD}/Accelerated-Lpbox-{ADMM}\}.},
	number = {{arXiv}:2207.02087},
	publisher = {{arXiv}},
	author = {Li, Longkang and Wu, Baoyuan},
	urldate = {2023-05-23},
	date = {2022-07-05},
	eprinttype = {arxiv},
	eprint = {2207.02087 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Computer Science - Discrete Mathematics, {GNN}, imitation learning},
	file = {arXiv Fulltext PDF:/home/brunompacheco/Zotero/storage/HMQHAM58/Li and Wu - 2022 - Learning to Accelerate Approximate Methods for Sol.pdf:application/pdf;arXiv.org Snapshot:/home/brunompacheco/Zotero/storage/3NQFMLW4/2207.html:text/html},
}

@inproceedings{deudon_learning_2018,
	location = {Cham},
	title = {Learning Heuristics for the {TSP} by Policy Gradient},
	isbn = {978-3-319-93031-2},
	doi = {10.1007/978-3-319-93031-2_12},
	series = {Lecture Notes in Computer Science},
	abstract = {The aim of the study is to provide interesting insights on how efficient machine learning algorithms could be adapted to solve combinatorial optimization problems in conjunction with existing heuristic procedures. More specifically, we extend the neural combinatorial optimization framework to solve the traveling salesman problem ({TSP}). In this framework, the city coordinates are used as inputs and the neural network is trained using reinforcement learning to predict a distribution over city permutations. Our proposed framework differs from the one in [1] since we do not make use of the Long Short-Term Memory ({LSTM}) architecture and we opted to design our own critic to compute a baseline for the tour length which results in more efficient learning. More importantly, we further enhance the solution approach with the well-known 2-opt heuristic. The results show that the performance of the proposed framework alone is generally as good as high performance heuristics ({OR}-Tools). When the framework is equipped with a simple 2-opt procedure, it could outperform such heuristics and achieve close to optimal results on 2D Euclidean graphs. This demonstrates that our approach based on machine learning techniques could learn good heuristics which, once being enhanced with a simple local search, yield promising results.},
	pages = {170--181},
	booktitle = {Integration of Constraint Programming, Artificial Intelligence, and Operations Research},
	publisher = {Springer International Publishing},
	author = {Deudon, Michel and Cournut, Pierre and Lacoste, Alexandre and Adulyasak, Yossiri and Rousseau, Louis-Martin},
	editor = {van Hoeve, Willem-Jan},
	date = {2018},
	langid = {english},
	keywords = {Neural networks, Reinforcement learning, Combinatorial optimization, Pred-and-opt, end-to-end, Policy gradient, Traveling salesman},
	file = {Deudon et al. - 2018 - Learning Heuristics for the TSP by Policy Gradient.pdf:/home/brunompacheco/Zotero/storage/8BBBBISZ/Deudon et al. - 2018 - Learning Heuristics for the TSP by Policy Gradient.pdf:application/pdf},
}

@article{smith_neural_1999,
	title = {Neural Networks for Combinatorial Optimization: A Review of More Than a Decade of Research},
	volume = {11},
	issn = {1091-9856},
	url = {https://pubsonline.informs.org/doi/10.1287/ijoc.11.1.15},
	doi = {10.1287/ijoc.11.1.15},
	shorttitle = {Neural Networks for Combinatorial Optimization},
	abstract = {It has been over a decade since neural networks were first applied to solve combinatorial optimization problems. During this period, enthusiasm has been erratic as new approaches are developed and (sometimes years later) their limitations are realized. This article briefly summarizes the work that has been done and presents the current standing of neural networks for combinatorial optimization by considering each of the major classes of combinatorial optimization problems. Areas which have not yet been studied are identified for future research.},
	pages = {15--34},
	number = {1},
	journaltitle = {{INFORMS} Journal on Computing},
	author = {Smith, Kate A.},
	urldate = {2023-06-12},
	date = {1999-02},
	note = {Publisher: {INFORMS}},
	keywords = {{SURVEY}, background, combinatorial optimization, neural networks},
	file = {Smith - 1999 - Neural Networks for Combinatorial Optimization A .pdf:/home/brunompacheco/Zotero/storage/FISARCH5/Smith - 1999 - Neural Networks for Combinatorial Optimization A .pdf:application/pdf},
}

@article{boschetti_matheuristics_2022,
	title = {Matheuristics: using mathematics for heuristic design},
	volume = {20},
	issn = {1614-2411},
	url = {https://doi.org/10.1007/s10288-022-00510-8},
	doi = {10.1007/s10288-022-00510-8},
	shorttitle = {Matheuristics},
	abstract = {Matheuristics are heuristic algorithms based on mathematical tools such as the ones provided by mathematical programming, that are structurally general enough to be applied to different problems with little adaptations to their abstract structure. The result can be metaheuristic hybrids having components derived from the mathematical model of the problems of interest, but the mathematical techniques themselves can define general heuristic solution frameworks. In this paper, we focus our attention on mathematical programming and its contributions to developing effective heuristics. We briefly describe the mathematical tools available and then some matheuristic approaches, reporting some representative examples from the literature. We also take the opportunity to provide some ideas for possible future development.},
	pages = {173--208},
	number = {2},
	journaltitle = {4OR},
	shortjournal = {4OR-Q J Oper Res},
	author = {Boschetti, Marco Antonio and Maniezzo, Vittorio},
	urldate = {2023-06-06},
	date = {2022-06-01},
	langid = {english},
	keywords = {90C11, 90-02, Mathematical programming, 90-00, Heuristics, Matheuristics},
	file = {Full Text PDF:/home/brunompacheco/Zotero/storage/AW3P4Y5Z/Boschetti and Maniezzo - 2022 - Matheuristics using mathematics for heuristic des.pdf:application/pdf},
}

@book{maniezzo_matheuristics_2021,
	location = {Cham, Switzerland},
	title = {Matheuristics: algorithms and implementations},
	isbn = {978-3-030-70277-9},
	shorttitle = {Matheuristics},
	abstract = {This book is the first comprehensive tutorial on matheuristics. Matheuristics are based on mathematical extensions of previously known heuristics, mainly metaheuristics, and on original, area-specific approaches. This tutorial provides a detailed discussion of both contributions, presenting the pseudocodes of over 40 algorithms, abundant literature references, and for each case a step-by-step description of a sample run on a common Generalized Assignment Problem example. C++ source codes of all algorithms are available in an associated {SW} repository},
	publisher = {Springer},
	author = {Maniezzo, Vittorio and Boschetti, Marco Antonio and Stützle, Thomas},
	date = {2021},
	note = {{OCLC}: 1249091063},
}

@book{goodfellow_deep_2016,
	title = {Deep Learning},
	publisher = {{MIT} Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	date = {2016},
}

@article{seman_energy-aware_2022,
	title = {An Energy-Aware Task Scheduling for Quality-of-Service Assurance in Constellations of Nanosatellites},
	volume = {22},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/10/3715},
	doi = {10.3390/s22103715},
	abstract = {When managing a constellation of nanosatellites, one may leverage this structure to improve the mission\&rsquo;s quality-of-service ({QoS}) by optimally distributing the tasks during an orbit. In this sense, this research proposes an offline energy-aware task scheduling problem formulation regarding the specifics of constellations, by considering whether the tasks are individual, collective, or stimulated to be redundant. By providing such an optimization framework, the idea of estimating an offline task schedule can serve as a baseline for the constellation design phase. For example, given a particular orbit, from the simulation of an irradiance model, the engineer can estimate how the mission value is affected by the inclusion or exclusion of individuals objects. The proposed model, given in the form of a multi-objective mixed-integer linear programming model, is illustrated in this work for several illustrative scenarios considering different sets of tasks and constellations. We also perform an analysis of the Pareto-optimal frontier of the problem, identifying the feasible trade-off points between constellation and individual tasks. This information can be useful to the decision-maker (mission operator) when planning the behavior in orbit.},
	number = {10},
	journaltitle = {Sensors},
	author = {Seman, Laio Oriel and Ribeiro, Brenda F. and Rigo, Cezar A. and Filho, Edemar Morsch and Camponogara, Eduardo and Leonardi, Rodrigo and Bezerra, Eduardo A.},
	date = {2022},
}

@article{rigo_branch-and-price_2022,
	title = {A branch-and-price algorithm for nanosatellite task scheduling to improve mission quality-of-service},
	volume = {303},
	issn = {0377-2217},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221722001606},
	doi = {https://doi.org/10.1016/j.ejor.2022.02.040},
	pages = {168--183},
	number = {1},
	journaltitle = {European Journal of Operational Research},
	author = {Rigo, Cezar Antônio and Seman, Laio Oriel and Camponogara, Eduardo and Filho, Edemar Morsch and Bezerra, Eduardo Augusto and Munari, Pedro},
	date = {2022},
	keywords = {Branch and price, Dantzig-Wolfe decomposition, Nanosatellite, Quality of service, Scheduling},
}

@article{muller_short-term_2022,
	title = {Short-term steady-state production optimization of offshore oil platforms: wells with dual completion (gas-lift and {ESP}) and flow assurance},
	volume = {30},
	issn = {1863-8279},
	url = {https://doi.org/10.1007/s11750-021-00604-2},
	doi = {10.1007/s11750-021-00604-2},
	shorttitle = {Short-term steady-state production optimization of offshore oil platforms},
	abstract = {Research on short-term steady-sate production optimization of oilfields led to the development of models and solution methods, several of which have found their way into practice. Early models considered satellite wells that operate with a fixed topside pressure and gas-lift injection, while recent approaches address distinct types of artificial lifting, pressure control, and processing equipment. By integrating existing approaches, this work presents a flexible model for production optimization that considers new features, including flow assurance constraints and smart selection of artificial lifting operating modes (gas-lift with multiple valves, electrical submersible pumping, and dual completion). Given that the proposed model is conceptual, piecewise-linear functions are obtained from field and simulation process data to approximate nonlinear relations. This way, the methodology decides the best combinations of routing and operation modes to maximize production gains. Simulated results are reported considering a representative asset that illustrates complex behavior.},
	pages = {152--180},
	number = {1},
	journaltitle = {{TOP}},
	shortjournal = {{TOP}},
	author = {Müller, Eduardo Rauh and Camponogara, Eduardo and Seman, Laio Oriel and Hülse, Eduardo Otte and Vieira, Bruno Ferreira and Miyatake, Luis Kin and Teixeira, Alex Furtado},
	urldate = {2023-03-27},
	date = {2022-04-01},
	langid = {english},
	keywords = {90-10, 90B30, 90C11, Dual well completion, Flow assurance, Mixed-integer optimization, Short-term production optimization, Steady-state optimization},
}

@article{mckelvey_review_1983,
	title = {Review of Urban Operations Research.},
	volume = {25},
	issn = {0036-1445},
	url = {https://www.jstor.org/stable/2030052},
	pages = {129--131},
	number = {1},
	journaltitle = {{SIAM} Review},
	author = {{McKelvey}, Robert and Nguyen, Hien},
	editora = {Larson, Richard C. and Odoni, Amedeo R.},
	editoratype = {collaborator},
	urldate = {2023-06-13},
	date = {1983},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	file = {JSTOR Full Text PDF:/home/brunompacheco/Zotero/storage/877RZ4SM/McKelvey and Nguyen - 1983 - Review of Urban Operations Research..pdf:application/pdf},
}

@book{larson_urban_1981,
	location = {Englewood Cliffs, N.J},
	title = {Urban operations research},
	isbn = {978-0-13-939447-8},
	pagetotal = {573},
	publisher = {Prentice-Hall},
	author = {Larson, Richard C. and Odoni, Amedeo R.},
	date = {1981},
	keywords = {Municipal services, Operations research, United States},
}

@book{noauthor_production_2006,
	title = {Production Planning by Mixed Integer Programming},
	isbn = {978-0-387-29959-4},
	url = {http://link.springer.com/10.1007/0-387-33477-7},
	series = {Springer Series in Operations Research and Financial Engineering},
	publisher = {Springer New York},
	urldate = {2023-06-13},
	date = {2006},
	langid = {english},
	doi = {10.1007/0-387-33477-7},
	file = {Full Text:/home/brunompacheco/Zotero/storage/RTM2GUUX/2006 - Production Planning by Mixed Integer Programming.pdf:application/pdf},
}

@book{pochet_production_2006,
	location = {New York Berlin},
	title = {Production planning by mixed integer programming},
	isbn = {978-0-387-33477-6},
	series = {Springer series in operations research and financial engineering},
	publisher = {Springer},
	author = {Pochet, Yves and Wolsey, Laurence A.},
	date = {2006},
}

@book{sawik_scheduling_2011,
	location = {Hoboken, N.J},
	title = {Scheduling in supply chains using mixed integer programming},
	isbn = {978-0-470-93573-6},
	pagetotal = {455},
	publisher = {Wiley},
	author = {Sawik, Tadeusz},
	date = {2011},
	note = {{OCLC}: ocn693553935},
	keywords = {Integer programming, Assembly-line methods, Business logistics, Data processing, Production scheduling},
}

@article{malandraki_time_1992,
	title = {Time Dependent Vehicle Routing Problems: Formulations, Properties and Heuristic Algorithms},
	volume = {26},
	issn = {0041-1655, 1526-5447},
	url = {https://pubsonline.informs.org/doi/10.1287/trsc.26.3.185},
	doi = {10.1287/trsc.26.3.185},
	shorttitle = {Time Dependent Vehicle Routing Problems},
	abstract = {The time dependent vehicle routing problem ({TDVRP}) is defined as follows. A vehicle fleet of fixed capacities serves customers of fixed demands from a central depot. Customers are assigned to vehicles and the vehicles routed so that the total time of the routes is minimized. The travel time between two customers or between a customer and the depot depends on the distance between the points and time of day. Time windows for serving the customers may also be present. The time dependent traveling salesman problem ({TDTSP}) is a special case of the {TDVRP} in which only one vehicle of infinite capacity is available. Mixed integer linear programming formulations of the {TDVRP} and the {TDTSP} are presented that treat the travel time functions as step functions. The characteristics and properties of the {TDVRP} preclude modification of most of the algorithms that have been developed for the vehicle routing problem. Several simple heuristic algorithms are given for the {TDTSP} and {TDVRP} without time windows based on the nearest-neighbor heuristic. A mathematical-programming-based heuristic for the {TDTSP} without time windows using cutting planes is also briefly discussed. Test results on small, randomly generated problems are reported.},
	pages = {185--200},
	number = {3},
	journaltitle = {Transportation Science},
	shortjournal = {Transportation Science},
	author = {Malandraki, Chryssi and Daskin, Mark S.},
	urldate = {2023-06-13},
	date = {1992-08},
	langid = {english},
}

@article{morrison_branch-and-bound_2016,
	title = {Branch-and-bound algorithms: A survey of recent advances in searching, branching, and pruning},
	volume = {19},
	issn = {1572-5286},
	url = {https://www.sciencedirect.com/science/article/pii/S1572528616000062},
	doi = {10.1016/j.disopt.2016.01.005},
	shorttitle = {Branch-and-bound algorithms},
	abstract = {The branch-and-bound (B\&B) algorithmic framework has been used successfully to find exact solutions for a wide array of optimization problems. B\&B uses a tree search strategy to implicitly enumerate all possible solutions to a given problem, applying pruning rules to eliminate regions of the search space that cannot lead to a better solution. There are three algorithmic components in B\&B that can be specified by the user to fine-tune the behavior of the algorithm. These components are the search strategy, the branching strategy, and the pruning rules. This survey presents a description of recent research advances in the design of B\&B algorithms, particularly with regards to these three components. Moreover, three future research directions are provided in order to motivate further exploration in these areas.},
	pages = {79--102},
	journaltitle = {Discrete Optimization},
	shortjournal = {Discrete Optimization},
	author = {Morrison, David R. and Jacobson, Sheldon H. and Sauppe, Jason J. and Sewell, Edward C.},
	urldate = {2023-06-13},
	date = {2016-02-01},
	langid = {english},
	keywords = {Integer programming, Branch-and-bound, Cyclic best first search, Discrete optimization, Search strategies, Survey},
	file = {ScienceDirect Full Text PDF:/home/brunompacheco/Zotero/storage/8FX83B63/Morrison et al. - 2016 - Branch-and-bound algorithms A survey of recent ad.pdf:application/pdf;ScienceDirect Snapshot:/home/brunompacheco/Zotero/storage/GADU3BDS/S1572528616000062.html:text/html},
}

@report{bestuzheva_scip_2021,
	title = {The {SCIP} Optimization Suite 8.0},
	url = {http://nbn-resolving.de/urn:nbn:de:0297-zib-85309},
	number = {21-41},
	institution = {Zuse Institute Berlin},
	type = {{ZIB}-Report},
	author = {Bestuzheva, Ksenia and Besançon, Mathieu and Chen, Wei-Kun and Chmiela, Antonia and Donkiewicz, Tim and Doornmalen, Jasper van and Eifler, Leon and Gaul, Oliver and Gamrath, Gerald and Gleixner, Ambros and Gottwald, Leona and Graczyk, Christoph and Halbig, Katrin and Hoen, Alexander and Hojny, Christopher and Hulst, Rolf van der and Koch, Thorsten and Lübbecke, Marco and Maher, Stephen J. and Matter, Frederic and Mühmer, Erik and Müller, Benjamin and Pfetsch, Marc E. and Rehfeldt, Daniel and Schlein, Steffan and Schlösser, Franziska and Serrano, Felipe and Shinano, Yuji and Sofranac, Boro and Turner, Mark and Vigerske, Stefan and Wegscheider, Fabian and Wellner, Philipp and Weninger, Dieter and Witzig, Jakob},
	date = {2021-12},
}

@incollection{land_automatic_2010,
	location = {Berlin, Heidelberg},
	title = {An Automatic Method for Solving Discrete Programming Problems},
	isbn = {978-3-540-68279-0},
	url = {https://doi.org/10.1007/978-3-540-68279-0_5},
	abstract = {In the late 1950s there was a group of teachers and research assistants at the London School of Economics interested in linear programming and its extensions, in particular Helen Makower, George Morton, Ailsa Land and Alison Doig. We had considered the ‘Laundry Van Problem’ until we discovered that it was known as the Traveling Salesman Problem, and had looked at aircraft timetabling, until quickly realizing that even the planning for the Scottish sector was beyond our capability! Alison Doig (now Harcourt) had studied the paper trim problem for her Masters project in Melbourne before coming to England.},
	pages = {105--132},
	booktitle = {50 Years of Integer Programming 1958-2008: From the Early Years to the State-of-the-Art},
	publisher = {Springer},
	author = {Land, Ailsa H. and Doig, Alison G.},
	editor = {Jünger, Michael and Liebling, Thomas M. and Naddef, Denis and Nemhauser, George L. and Pulleyblank, William R. and Reinelt, Gerhard and Rinaldi, Giovanni and Wolsey, Laurence A.},
	urldate = {2023-06-13},
	date = {2010},
	langid = {english},
	doi = {10.1007/978-3-540-68279-0_5},
	keywords = {British Petroleum, Discrete Variable, London School, Storage Tank, Travel Salesman Problem},
	file = {Full Text PDF:/home/brunompacheco/Zotero/storage/ZXRYH8SR/Land and Doig - 2010 - An Automatic Method for Solving Discrete Programmi.pdf:application/pdf},
}

@article{padberg_branch-and-cut_1991,
	title = {A Branch-and-Cut Algorithm for the Resolution of Large-Scale Symmetric Traveling Salesman Problems},
	volume = {33},
	issn = {0036-1445, 1095-7200},
	url = {http://epubs.siam.org/doi/10.1137/1033004},
	doi = {10.1137/1033004},
	pages = {60--100},
	number = {1},
	journaltitle = {{SIAM} Review},
	shortjournal = {{SIAM} Rev.},
	author = {Padberg, Manfred and Rinaldi, Giovanni},
	urldate = {2023-06-13},
	date = {1991-03},
	langid = {english},
}

@article{savelsbergh_branch-and-price_1997,
	title = {A Branch-and-Price Algorithm for the Generalized Assignment Problem},
	volume = {45},
	issn = {0030-364X, 1526-5463},
	url = {https://pubsonline.informs.org/doi/10.1287/opre.45.6.831},
	doi = {10.1287/opre.45.6.831},
	abstract = {The generalized assignment problem examines the maximum profit assignment of jobs to agents such that each job is assigned to precisely one agent subject to capacity restrictions on the agents. A new algorithm for the generalized assignment problem is presented that employs both column generation and branch-and-bound to obtain optimal integer solutions to a set partitioning formulation of the problem.},
	pages = {831--841},
	number = {6},
	journaltitle = {Operations Research},
	shortjournal = {Operations Research},
	author = {Savelsbergh, Martin},
	urldate = {2023-06-13},
	date = {1997-12},
	langid = {english},
}

@article{grimstad_relu_2019,
	title = {{ReLU} networks as surrogate models in mixed-integer linear programs},
	volume = {131},
	issn = {00981354},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0098135419307203},
	doi = {10.1016/j.compchemeng.2019.106580},
	pages = {106580},
	journaltitle = {Computers \& Chemical Engineering},
	shortjournal = {Computers \& Chemical Engineering},
	author = {Grimstad, Bjarne and Andersson, Henrik},
	urldate = {2023-06-13},
	date = {2019-12},
	langid = {english},
	file = {Submitted Version:/home/brunompacheco/Zotero/storage/XMAEJHPR/Grimstad and Andersson - 2019 - ReLU networks as surrogate models in mixed-integer.pdf:application/pdf},
}

@article{dikin_iterative_1967,
	title = {Iterative solution of problems of linear and quadratic programming},
	volume = {8},
	issn = {0197-6788},
	pages = {674--675},
	journaltitle = {Soviet Mathematics. Doklady},
	shortjournal = {Sov. Math., Dokl.},
	author = {Dikin, I. I.},
	date = {1967},
	keywords = {65K10, 90C05, 90C20},
	file = {Snapshot:/home/brunompacheco/Zotero/storage/JWSEAMA2/zbmath.org.html:text/html},
}
