<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>ML4CO - Part 1: A brief overview of machine learning for combinatorial optimization | Bruno M. Pacheco</title>
    <meta name="author" content="Bruno M. Pacheco">
    <meta name="description" content="The big picture on machine learning applications as heuristics for combinatorial optimization.">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%93&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://brunompacheco.github.io/blog/2023/literature-review-ml4co-1/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "ML4CO - Part 1: A brief overview of machine learning for combinatorial optimization",
      "description": "The big picture on machine learning applications as heuristics for combinatorial optimization.",
      "published": "June 14, 2023",
      "authors": [
        {
          "author": "Bruno M. Pacheco",
          "authorURL": "",
          "affiliations": [
            {
              "name": "DAS, UFSC",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Bruno </span>M. Pacheco</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>ML4CO - Part 1: A brief overview of machine learning for combinatorial optimization</h1>
        <p>The big picture on machine learning applications as heuristics for combinatorial optimization.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#algorithm-selection">Algorithm selection</a></div>
            <div><a href="#learning-based-heuristics">Learning-based heuristics</a></div>
            <div><a href="#training-ml-models">Training ML models</a></div>
            <ul>
              <li><a href="#supervised-learning">Supervised learning</a></li>
              <li><a href="#reinforcement-learning">Reinforcement learning</a></li>
              
            </ul>
<div><a href="#challenges">Challenges</a></div>
            <ul>
              <li><a href="#data-generation">Data generation</a></li>
              <li><a href="#guarantees">Guarantees</a></li>
              <li><a href="#problem-size">Problem size</a></li>
              
            </ul>
          </nav>
        </d-contents>

        <!-- TODO: find a better way to highlight this first section -->
<p><em>This text is a direct result from the literature review I performed for my Master’s.
The main goal of this first part is to provide context and background for the following review on end-to-end learning-based heuristics for MILP.
This is largely based on the work by Bengio et al.<d-cite key="bengio_machine_2021"></d-cite>, so I recommend it to the reader that wants to find more detailed information and more references.</em></p>

<!-- ###  ‎ -->
<p><br></p>

<p>Let us suppose that a delivery company must plan the route for a carrier given a set of packages.
The route must take the carrier through the recipients of every package and back to the company headquarters.
As there are finitely many packages, there are finitely many possible routes.
To find the optimal route, you can write a computer program that evaluates all possible routes and returns the one with the smallest cost.
It is easy to see that this program will always (eventually) finish and will return the optimal solution.
However, as the number of solutions (routes) grows exponentially with the size of the problem (number of recipients/packages), for a large-enough number of deliveries, committing to a random route will probably be faster than waiting for the program to finish.
This example is a classic occurrence of the Traveling Salesperson Problem (TSP), a widely studied combinatorial optimization (CO) problem.</p>

<p>CO problems are hard.
In fact, CO is often used to refer to NP-hard integer optimization problems.
But despite being NP-hard, many CO problems are solved within reasonable time even for millions of variables (and constraints).
This is usually due to experts being able to exploit structures of the problem, creating efficient heuristics.</p>

<p>In the delivery company example, suppose that you are quite familiar with the location of the deliveries.
You modify the computer program, such that instead of iterating over all possible solutions, you remove routes that pass through residential, low-speed areas and prioritize routes that use high-speed roads.
Your new computer program will not iterate over all possible routes, so it may not find the optimal solution, but it will probably give you a very good route much quicker than the previous one.
Furthermore, instead of writing a computer program, you can let an experienced carrier guess a route based on their previous deliveries.
The guess probably won’t be the best route, but it may be good enough.</p>

<p>Sadly, the expert knowledge to develop heuristics for CO problems takes a lot of effort to develop and may result in heuristics that are not cheap to compute or easy to implement.
Machine learning (ML) techniques, on the other hand, seem like the perfect fit for such heuristics, as they do not require an expert and are really fast to compute.
On top of that, deep learning has shown great results applied to high-dimensional structured data such as image, proteins, and text, which makes us think that it could provide great results as well when applied to CO problem instances.</p>

<p>For example, computing which routes pass by low-speed zones and which do not may be as expensive as finding a good route.
At the same time, historical data on past routes along with the speed at each section can be used to train a classifier that determines whether a new route is slow or not.
Furthermore, instead of embedding the graph-like traffic data into a low-dimensional, tabular format for machine learning models, one can use a graph neural network as the classifier.</p>

<h2 id="algorithm-selection">Algorithm selection</h2>

<p>A CO problem is a problem of minimizing an <em>objective function</em> given a finite set of <em>feasible solutions</em>.
Given an algorithm for solving CO problems, its quality is usually determined from its computational complexity and experimental results on benchmark instances.
However, an algorithm that works well on some classes of CO problems may not work well on others.
Thus, practitioners end up having to experiment between the alternatives and, if none fulfills the requirements, having to adjust configurations, exploring the problem’s structure, manually building heuristics, etc.</p>

<p>To better grasp the challenge of selecting an algorithm for a realistic application, let us take the delivery company example of the introduction.
We are going to suppose that the desired computer program must work only for problems on a given city and that the origin (company headquarters) is always the same.
The goal of the company is to find a software that can find a good route within a time limit.
Instead of picking a solver based on benchmarks, you test a few different solvers based on historical data (past orders).
As your instances are too large, none of the algorithms can find a good solution on time given their default setting, so you explore different configurations, tweaking the parameters in the hope of improving their performance.
Furthermore, you notice that the recipients are often grouped in certain regions, so you design an algorithm that first groups nearby recipients in a single vertex, solves this simplified problem, and then finds a route within each group of recipients given the outcome of the simplified problem.</p>

<p>We can provide a mathematical formulation for the problem of finding a good algorithm for the problem of interest.
Let \(\mathcal{I}\) be the set of all instances of interest and \(P\) be a probability distribution over \(\mathcal{I}\).
Let \(\mathcal{A}\) be the set of all algorithms that can solve instances of the problem of interest and \(m : \mathcal{I}\times \mathcal{A}\to \mathbb{R}\) be a performance metric
For convenience, we will assume that, for any \(a_1,a_2 \in \mathcal{A}\) and \(I \in  \mathcal{I}\), then \(m\left( I,a_1 \right) &gt; m\left( I,a_2 \right)\) implies that \(a_1\) outperforms \(a_2\) in instance \(I\).
The problem of finding the best algorithm can be described as</p>

\[\max_{a\in \mathcal{A}} \, \mathbb{E}_{I\sim P } m(I,a)
.\]

<p>As this is usually impossible to compute, one can use the approximation based on a dataset \(\mathcal{D}\) of instances independently drawn from \(P\).
The problem, then, becomes</p>

\[\max_{a\in \mathcal{A}} \, \frac{1}{|\mathcal{D}|} \sum_{I\in \mathcal{D}} m(I,a)
.\]

<h2 id="learning-based-heuristics">Learning-based heuristics</h2>

<p>When we consider CO algorithms enhanced with machine learning models, the comparison is often over an uncountable set of algorithms.
For example, let \(\Theta\) be the parameter space of the ML models, and \(\mathcal{A}=\left\{ a_\theta : \theta\in \Theta \right\}\), i.e., the algorithms are defined by the ML model’s parameters.
The problem of selecting the best algorithm can be written</p>

\[\max_{\theta\in \Theta} \, \frac{1}{|\mathcal{D}|} \sum_{I\in \mathcal{D}} m(I,a_\theta)
.\]

<p>In other words, rather than searching for the best algorithm, one searches for the best vector of parameters.</p>

<p>With respect to how ML models can be used in algorithms for CO, Bengio et al.<d-cite key="bengio_machine_2021"></d-cite> proposed three categories of learning-based heuristics.
Even though the categorization is not exhaustive (as will be seen later on), it is useful to show the possibilities that exist for applying ML models.</p>

<p>At the “deepest” level, an ML model can be trained to take decisions within CO solvers, replacing costly computations or in the place of already existing heuristics within the solver.
An example of this approach can be seen in Nair et al.<d-cite key="nair_solving_2021"></d-cite>, where the authors trained a deep learning model to select between variables for branching, within a branch-and-bound algorithm.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bengio_aoa-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bengio_aoa-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bengio_aoa-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/bengio_aoa.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Diagram of a ML model being used to take decisions within a CO solver (<em>OR</em> block). Image from Bengio et al.<d-cite key="bengio_machine_2021"></d-cite>.
</div>

<p>The second category comprises heuristics with ML models being called to take decisions prior to the execution of the CO solvers.
In this approach, the ML model’s output helps to define the information provided to the CO solver.
In Kruber et al.<d-cite key="kruber_learning_2017"></d-cite>, the authors trained a model to decide whether to apply a Dantzig-Wolfe decomposition to reformulate an instance or not, based on the predicted running time reduction of the solver.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bengio_l2c-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bengio_l2c-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bengio_l2c-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/bengio_l2c.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Diagram of a ML model being used to enhance the information provided to a CO solver (<em>OR</em> block). Image from Bengio et al.<d-cite key="bengio_machine_2021"></d-cite>.
</div>

<p>Finally, ML models can be trained to predict a solution based on the information of an instance, which will be referred to as an <em>end-to-end</em> approach.
An example is the work by Vinyals et al.<d-cite key="vinyals_pointer_2015"></d-cite>, in which the authors propose a novel deep learning model capable of providing feasible solutions to the TSP.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bengio_e2e-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bengio_e2e-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bengio_e2e-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/bengio_e2e.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Diagram of a ML model being used as a heuristic by itself, i.e., without calling an optimization algorithm. This is the end-to-end approach. Image from Bengio et al.<d-cite key="bengio_machine_2021"></d-cite>.
</div>

<p>Note that end-to-end models, beside being trained to predict a solution, can be used in different settings.
For example, the model’s output (a candidate solution) can be used to define a region for proximity search, as done by Han et al.<d-cite key="han_gnn-guided_2023"></d-cite>.</p>

<h2 id="training-ml-models">Training ML models</h2>

<h3 id="supervised-learning">Supervised learning</h3>

<p>For any given structure of CO learning-based heuristic, training through supervision requires us to provide data on the inputs and expected outputs for the model.
We assume that the training data was generated by an “expert”, which the model will learn to imitate.
Therefore, if we provide the model with the optimal solutions as targets, the model will learn to solve the optimization problems.</p>

<p>Supervised learning is possible whenever we have either observations on the expert (e.g., historical performance of a human operator), or access to a data generation process (e.g., artificial instances of the problem and targets given by a solver).
However, supervised learning is mostly adequate to problems in which the expert is not suitable for the application, e.g., because it is too expensive to compute, once the ML model’s performance will, at best, be <em>on par</em> with the expert’s performance.</p>

<p>In Gasse et al.<d-cite key="gasse_exact_2019"></d-cite>, the authors take as an expert the strong branching rule, which is known to provide good results in branch-and-bound, but is expensive to compute.
The data is generated beforehand and fed to the training algorithm as input-output pairs, which guides the ML model towards imitating the rule.</p>

<h3 id="reinforcement-learning">Reinforcement learning</h3>

<p>Instead of teaching the model the desired behavior (targets), we can let the model learn by trial-and-error.
More precisely, the model can be seen as an <em>agent</em> that modifies the <em>state</em> of an <em>environment</em> (e.g., current node of a branch-and-bound tree) through <em>actions</em> (e.g., variable selection for branching).
For the model to learn, we need to provide a function that <em>rewards</em> the model if the desired states are achieved or the right actions are performed.</p>

<p>To use reinforcement learning, therefore, we do not need the expected outputs of the model.
If the reward signal matches the optimization goal, the model will learn to solve the optimization problems without ever being told what the solutions are.
Note that this allows for the model to outperform any known method.
However, as there are usually many possible actions (dimensionality of the model’s output) and states, the learning problem quickly becomes intractable.</p>

<p>In contrast to the example from the previous section (Supervised learning), in Etheve et al.<d-cite key="etheve_reinforcement_2020"></d-cite>, the authors propose to learn branching from scratch.
They adapt reinforcement learning techniques (Q-learning) and propose an approach specific for the branch-and-bound setting.
Through the proposed framework, the model learns to minimize the size of the branch-and-bound tree (global metric).
Notably, all evaluated frameworks require thousands of iterations to achieve competitive performance, in which each iteration requires the optimization of hundreds of CO problems, i.e., the computational cost for training is high even for problems of modest size.</p>

<h2 id="challenges">Challenges</h2>

<p>Regardless of the heuristic configuration or the training approach, developing machine learning models for CO problems faces challenges.</p>

<h3 id="data-generation">Data generation</h3>

<p>The performance of learning algorithms depends heavily on the data available, both in quantity and in quality.
<!-- To have a reliable estimation of the generalization error, it is essential that the test set is drawn from a realistic distribution.
The same is true for the training set, but with respect to reducing the generalization error. -->
The most straight-forward way to build the datasets is to sample from historical data.
In the delivery company example of the introduction, this is equivalent to acquiring the problems and the routes of past deliveries.
Although a valid approach for collecting realistic instances of CO problems, sampling from historical data limits the performance (on training and evaluation) to that of the previous “expert”.
Furthermore, applications that have large-enough historical data are usually those for which a fast-enough solution already exists, limiting the gains to computational speed-ups.</p>

<p>A more general scenario is to have to generate problem instances for training and testing.
<!-- This scenario usually inccurs in high development costs, as finding good solutions (even when just for the test set, in the case of RL) is computationally costly for problems of practical interest.
Furthermore, even determining whether a problem instance is feasible or not is generally itself an NP-hard problem.
On top of that, we known that (assuming NP $$\neq$$ co-NP) polynomial-time instance generation methods for CO problems actually sample from easier sub-problems<d-cite key="yehuda_2020"></d-cite>. -->
Some interesting references on generating instances of optimization problems can be seen in the works by Smith-Miles et al.<d-cite key="smith-miles_generating_2015"></d-cite> and Malitsky et al.<d-cite key="malitsky_structure-preserving_2016"></d-cite>.</p>

<!-- TODO -->
<!-- ### Dimensionality

CO problems have high dimensionality and are highly structured.
=> the higher the number of dimensions, exponentially more data is necessary for learning
Traditional ML models are not built to deal with the structures within CO problems, e.g., TSP (invariant to graph definition). -->

<h3 id="guarantees">Guarantees</h3>

<p>Opting for a heuristic solution (usually) implies in abdicating from optimality guarantees.
However, with machine learning, even feasibilty is often off the table, as constraining the model’s output is usually impossible.</p>

<p>This is not impactful on applications in which the model is used to select between optimality/feasibility-preserving choices.
For example, Liberto et al.<d-cite key="liberto_dash_2016"></d-cite> train a model to select between branching strategies.
However, if the model must provide a candidate solution or a valid constriant, it is often the case that the best we can do is to teach the model to respect feasibility constraints, e.g., through lagrangian regularization.</p>

<h3 id="problem-size">Problem size</h3>

<p>Problems of interest for heuristics are often very large.
As discussed before, larger problems imply in higher costs for generating feasible instances and finding good solutions.
However, large problems also imply in higher dimensionality, which increases (exponentially) the expected training set size for achieving satisfactory performance, and also the trianing cost.
This can be alleviated by exploring symmetries of the problem or working in the embedding of the instance.
For example, many works have exploited the efficiency of graph neural networks by embedding the instances as graphs.</p>

<!-- Maybe add a conclusion discussing the forthcomings? I believe I could, inspired by Smith, 1999, describe why ML+CO is attracting plenty of attention now (in comparison to the 90s), and use this as a hook to Part 2, because of structured models/GNNs. -->

<!-- ###  ‎ -->
<p><br></p>

<p><em>That was it for this first part.
In part 2, I will dive deeper into end-to-end models trained through supervision, and end-to-end heuristics using these models.</em></p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/ML2MILP.bib"></d-bibliography>
</div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        © Copyright 2023 Bruno M. Pacheco. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. # keywords: academic-website, research, machine-learning, deep-learning, optimization, integer-programming, MILP
Last updated: October 02, 2023.
      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
